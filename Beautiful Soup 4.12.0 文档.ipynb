{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beautiful Soup 4.12.0 文档\n",
    "\n",
    "[Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/) 是一个可以从HTML或XML文件中提取数据的Python库.它能够通过你喜欢的转换器实现惯用的文档导航,查找,修改文档的方式.Beautiful Soup会帮你节省数小时甚至数天的工作时间.\n",
    "\n",
    "这篇文档介绍了BeautifulSoup4中所有主要特性,并且有小例子.让我来向你展示它适合做什么,如何工作,怎样使用,如何达到你想要的效果,和处理异常情况.\n",
    "\n",
    "文档中出现的例子在Python2.7和Python3.2中的执行结果相同\n",
    "\n",
    "你可能在寻找 [Beautiful Soup3](http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html) 的文档,Beautiful Soup 3 目前已经停止开发,我们推荐在现在的项目中使用Beautiful Soup 4, [移植到BS4](http://www.baidu.com/)\n",
    "\n",
    "这篇帮助文档已经被翻译成了其它语言:\n",
    "\n",
    "- [这篇文档当然还有中文版.](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/)\n",
    "- このページは日本語で利用できます([外部リンク](http://kondou.com/BS4/))\n",
    "- [이 문서는 한국어 번역도 가능합니다.](https://www.crummy.com/software/BeautifulSoup/bs4/doc.ko/)\n",
    "- [Este documento também está disponível em Português do Brasil.](https://www.crummy.com/software/BeautifulSoup/bs4/doc.ptbr/)\n",
    "- [Эта документация доступна на русском языке.](https://www.crummy.com/software/BeautifulSoup/bs4/doc.ru/)\n",
    "\n",
    "## 寻求帮助\n",
    "\n",
    "如果你有关于BeautifulSoup的问题,可以发送邮件到 [讨论组](https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup) .如果你的问题包含了一段需要转换的HTML代码,那么确保你提的问题描述中附带这段HTML文档的 [代码诊断](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id67) [[1\\]](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id89)\n",
    "\n",
    "# 快速开始\n",
    "\n",
    "下面的一段HTML代码将作为例子被多次用到.这是 *爱丽丝梦游仙境的* 的一段内容(以后内容中简称为 *爱丽丝* 的文档):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用BeautifulSoup解析这段代码,能够得到一个 `BeautifulSoup` 的对象,并能按照标准的缩进格式的结构输出:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   The Dormouse's story\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p class=\"title\">\n",
      "   <b>\n",
      "    The Dormouse's story\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   Once upon a time there were three little sisters; and their names were\n",
      "   <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n",
      "    Elsie\n",
      "   </a>\n",
      "   ,\n",
      "   <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\n",
      "    Lacie\n",
      "   </a>\n",
      "   and\n",
      "   <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">\n",
      "    Tillie\n",
      "   </a>\n",
      "   ;\n",
      "and they lived at the bottom of a well.\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   ...\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "几个简单的浏览结构化数据的方法:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>The Dormouse's story</title>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'title'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"The Dormouse's story\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'head'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<p class=\"title\"><b>The Dormouse's story</b></p>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['title']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title\n",
    "# <title>The Dormouse's story</title>\n",
    "\n",
    "soup.title.name\n",
    "# u'title'\n",
    "\n",
    "soup.title.string\n",
    "# u'The Dormouse's story'\n",
    "\n",
    "soup.title.parent.name\n",
    "# u'head'\n",
    "\n",
    "soup.p\n",
    "# <p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "soup.p['class']\n",
    "# u'title'\n",
    "\n",
    "soup.a\n",
    "# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
    "\n",
    "soup.find_all('a')\n",
    "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
    "\n",
    "soup.find(id=\"link3\")\n",
    "# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从文档中找到所有<a>标签的链接:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://example.com/elsie\n",
      "http://example.com/lacie\n",
      "http://example.com/tillie\n"
     ]
    }
   ],
   "source": [
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))\n",
    "    # http://example.com/elsie\n",
    "    # http://example.com/lacie\n",
    "    # http://example.com/tillie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从文档中获取所有文字内容:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Dormouse's story\n",
      "\n",
      "The Dormouse's story\n",
      "Once upon a time there were three little sisters; and their names were\n",
      "Elsie,\n",
      "Lacie and\n",
      "Tillie;\n",
      "and they lived at the bottom of a well.\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.get_text())\n",
    "# The Dormouse's story\n",
    "#\n",
    "# The Dormouse's story\n",
    "#\n",
    "# Once upon a time there were three little sisters; and their names were\n",
    "# Elsie,\n",
    "# Lacie and\n",
    "# Tillie;\n",
    "# and they lived at the bottom of a well.\n",
    "#\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是你想要的吗?别着急,还有更好用的\n",
    "\n",
    "# 安装 Beautiful Soup\n",
    "\n",
    "如果你用的是新版的Debain或ubuntu,那么可以通过系统的软件包管理来安装:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ apt-get install Python-bs4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful Soup 4 通过PyPi发布,所以如果你无法使用系统包管理安装,那么也可以通过 `easy_install` 或 `pip` 来安装.包的名字是 `beautifulsoup4` ,这个包兼容Python2和Python3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ easy_install beautifulsoup4\n",
    "\n",
    "$ pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(在PyPi中还有一个名字是 `BeautifulSoup` 的包,但那可能不是你想要的,那是 [Beautiful Soup3](http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html) 的发布版本,因为很多项目还在使用BS3, 所以 `BeautifulSoup` 包依然有效.但是如果你在编写新项目,那么你应该安装的 `beautifulsoup4` )\n",
    "\n",
    "如果你没有安装 `easy_install` 或 `pip` ,那你也可以 [下载BS4的源码](http://www.crummy.com/software/BeautifulSoup/download/4.x/) ,然后通过setup.py来安装.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安装解析器\n",
    "\n",
    "Beautiful Soup支持Python标准库中的HTML解析器,还支持一些第三方的解析器,其中一个是 [lxml](http://lxml.de/) .根据操作系统不同,可以选择下列方法来安装lxml:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ apt-get install Python-lxml\n",
    "$ easy_install lxml\n",
    "$ pip install lxml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一个可供选择的解析器是纯Python实现的 [html5lib](http://code.google.com/p/html5lib/) , html5lib的解析方式与浏览器相同,可以选择下列方法来安装html5lib:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ apt-get install Python-html5lib\n",
    "$ easy_install html5lib\n",
    "$ pip install html5lib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下表列出了主要的解析器,以及它们的优缺点:\n",
    "\n",
    "| 解析器           | 使用方法                                                     | 优势                                                  | 劣势                                            |\n",
    "| :--------------- | :----------------------------------------------------------- | :---------------------------------------------------- | :---------------------------------------------- |\n",
    "| Python标准库     | `BeautifulSoup(markup, \"html.parser\")`                       | Python的内置标准库执行速度适中文档容错能力强          | Python 2.7.3 or 3.2.2)前 的版本中文档容错能力差 |\n",
    "| lxml HTML 解析器 | `BeautifulSoup(markup, \"lxml\")`                              | 速度快文档容错能力强                                  | 需要安装C语言库                                 |\n",
    "| lxml XML 解析器  | `BeautifulSoup(markup, [\"lxml-xml\"])``BeautifulSoup(markup, \"xml\")` | 速度快唯一支持XML的解析器                             | 需要安装C语言库                                 |\n",
    "| html5lib         | `BeautifulSoup(markup, \"html5lib\")`                          | 最好的容错性以浏览器的方式解析文档生成HTML5格式的文档 | 速度慢不依赖外部扩展                            |\n",
    "\n",
    "推荐使用lxml作为解析器,因为效率更高. 在Python2.7.3之前的版本和Python3中3.2.2之前的版本,必须安装lxml或html5lib, 因为那些Python版本的标准库中内置的HTML解析方法不够稳定.\n",
    "\n",
    "提示: 如果一段HTML或XML文档格式不正确的话,那么在不同的解析器中返回的结果可能是不一样的,查看 [解析器之间的区别](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id54) 了解更多细节\n",
    "\n",
    "# 如何使用\n",
    "\n",
    "将一段文档传入BeautifulSoup 的构造方法,就能得到一个文档的对象, 可以传入一段字符串或一个文件句柄.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(open(\"index.html\"))\n",
    "\n",
    "soup = BeautifulSoup(\"<html>data</html>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先,文档被转换成Unicode,并且HTML的实例都被转换成Unicode编码\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><body><p>Sacré bleu!</p></body></html>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BeautifulSoup(\"Sacr&eacute; bleu!\")\n",
    "# <html><head></head><body>Sacré bleu!</body></html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后,Beautiful Soup选择最合适的解析器来解析这段文档,如果手动指定解析器那么Beautiful Soup会选择指定的解析器来解析文档.(参考 [解析成XML](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#xml) ).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 对象的种类\n",
    "\n",
    "Beautiful Soup 将复杂的 HTML 文档转换为复杂的 Python 对象树。但您只需要处理大约四种对象：  `Tag` , `NavigableString` , `BeautifulSoup` , `Comment` .\n",
    "\n",
    "## class bs4.Tag \n",
    "\n",
    "`Tag` 对象与XML或HTML原生文档中的tag相同:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup('<b class=\"boldest\">Extremely bold</b>')\n",
    "tag = soup.b\n",
    "type(tag)\n",
    "# <class 'bs4.element.Tag'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tag有很多方法和属性,在 [遍历文档树](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id19) 和 [搜索文档树](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id28) 中有详细解释.现在介绍一下tag中最重要的属性: name和attributes\n",
    "\n",
    "### Name\n",
    "\n",
    "每个tag都有自己的名字,通过 `.name` 来获取:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag.name\n",
    "# u'b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果改变了tag的name,那将影响所有通过当前Beautiful Soup对象生成的HTML文档:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<blockquote class=\"boldest\">Extremely bold</blockquote>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag.name = \"blockquote\"\n",
    "tag\n",
    "# <blockquote class=\"boldest\">Extremely bold</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes\n",
    "\n",
    "一个tag可能有很多个属性. tag `<b class=\"boldest\">` 有一个 “class” 的属性,值为 “boldest” . tag的属性的操作方法与字典相同:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boldest']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag['class']\n",
    "# u'boldest'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以直接”点”取属性, 比如: `.attrs` :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class': ['boldest']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag.attrs\n",
    "# {u'class': u'boldest'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tag的属性可以被添加,删除或修改. 再说一次, tag的属性操作方法与字典一样\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<blockquote class=\"verybold\" id=\"1\">Extremely bold</blockquote>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<blockquote>Extremely bold</blockquote>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "tag['class'] = 'verybold'\n",
    "tag['id'] = 1\n",
    "tag\n",
    "# <blockquote class=\"verybold\" id=\"1\">Extremely bold</blockquote>\n",
    "\n",
    "del tag['class']\n",
    "del tag['id']\n",
    "tag\n",
    "# <blockquote>Extremely bold</blockquote>\n",
    "\n",
    "# tag['class']\n",
    "# KeyError: 'class'\n",
    "print(tag.get('class')) # 用dict.get 不会报错\n",
    "# None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 多值属性\n",
    "\n",
    "HTML 4定义了一系列可以包含多个值的属性.在HTML5中移除了一些,却增加更多.最常见的多值的属性是 class (一个tag可以有多个CSS的class). 还有一些属性 `rel` , `rev` , `accept-charset` , `headers` , `accesskey` . 在Beautiful Soup中多值属性的返回类型是list:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['body', 'strikeout']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['body']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "css_soup = BeautifulSoup('<p class=\"body strikeout\"></p>')\n",
    "css_soup.p['class']\n",
    "# [\"body\", \"strikeout\"]\n",
    "\n",
    "css_soup = BeautifulSoup('<p class=\"body\"></p>')\n",
    "css_soup.p['class']\n",
    "# [\"body\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果某个属性看起来好像有多个值,但在任何版本的HTML定义中都没有被定义为多值属性,那么Beautiful Soup会将这个属性作为字符串返回\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my id'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_soup = BeautifulSoup('<p id=\"my id\"></p>')\n",
    "id_soup.p['id']\n",
    "# 'my id'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将tag转换成字符串时,多值属性会合并为一个值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['index']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>Back to the <a rel=\"index contents\">homepage</a></p>\n"
     ]
    }
   ],
   "source": [
    "rel_soup = BeautifulSoup('<p>Back to the <a rel=\"index\">homepage</a></p>')\n",
    "rel_soup.a['rel']\n",
    "# ['index']\n",
    "rel_soup.a['rel'] = ['index', 'contents']\n",
    "print(rel_soup.p)\n",
    "# <p>Back to the <a rel=\"index contents\">homepage</a></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果转换的文档是XML格式,那么tag中不包含多值属性(作为带空格的字符串返回)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'body strikeout'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_soup = BeautifulSoup('<p class=\"body strikeout\"></p>', 'xml')\n",
    "xml_soup.p['class']\n",
    "# u'body strikeout'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class bs4.NavigableString 可以遍历的字符串\n",
    "\n",
    "字符串常被包含在tag内.Beautiful Soup用 `NavigableString` 类来包装tag中的字符串:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extremely bold'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "bs4.element.NavigableString"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag.string\n",
    "# u'Extremely bold'\n",
    "type(tag.string)\n",
    "# <class 'bs4.element.NavigableString'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个 `NavigableString` 字符串与Python中的Unicode字符串相同,并且还支持包含在 [遍历文档树](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id19) 和 [搜索文档树](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id28) 中的一些特性. 通过 `unicode()` 方法可以直接将 `NavigableString` 对象转换成Unicode字符串:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extremely bold'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在Python 3中，unicode已重命名为str。\n",
    "unicode_string = str(tag.string)\n",
    "unicode_string\n",
    "# u'Extremely bold'\n",
    "\n",
    "type(unicode_string)\n",
    "# <type 'unicode'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tag中包含的字符串不能编辑,但是可以被替换成其它的字符串,用 [replace_with()](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#replace-with) 方法:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extremely bold'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<blockquote>No longer bold</blockquote>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag.string.replace_with(\"No longer bold\")\n",
    "tag\n",
    "# <blockquote>No longer bold</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NavigableString` 对象支持 [遍历文档树](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id19) 和 [搜索文档树](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id28) 中定义的大部分属性, 并非全部.尤其是,一个字符串不能包含其它内容(tag能够包含字符串或是其它tag),字符串不支持 `.contents` 或 `.string` 属性或 `find()` 方法.\n",
    "\n",
    "如果想在Beautiful Soup之外使用 `NavigableString` 对象,需要调用 `unicode()` 方法,将该对象转换成普通的Unicode字符串,否则就算Beautiful Soup已方法已经执行结束,该对象的输出也会带有对象的引用地址.这样会浪费内存.\n",
    "\n",
    "## class bs4.BeautifulSoup¶\n",
    "\n",
    "`BeautifulSoup` 对象表示的是一个文档的全部内容.大部分时候,可以把它当作 `Tag` 对象,它支持 [遍历文档树](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id19) 和 [搜索文档树](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id28) 中描述的大部分的方法.\n",
    "\n",
    "因为 `BeautifulSoup` 对象并不是真正的HTML或XML的tag,所以它没有name和attribute属性.但有时查看它的 `.name` 属性是很方便的,所以 `BeautifulSoup` 对象包含了一个值为 “[document]” 的特殊属性 `.name`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[document]'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup('<b class=\"boldest\">Extremely bold</b>')\n",
    "soup.name\n",
    "\n",
    "# u'[document]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special strings 特殊字符串\n",
    "\n",
    "注释及特殊字符串\n",
    "\n",
    "`Tag` , `NavigableString` , `BeautifulSoup` 几乎覆盖了html和xml中的所有内容,但是还有一些特殊对象.容易让人担心的内容是文档的注释部分:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Comment"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markup = \"<b><!--Hey, buddy. Want to buy a used parser?--></b>\"\n",
    "soup = BeautifulSoup(markup)\n",
    "comment = soup.b.string\n",
    "type(comment)\n",
    "# <class 'bs4.element.Comment'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class bs4.Comment\n",
    "\n",
    "`Comment` 对象是一个特殊类型的 `NavigableString` 对象:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey, buddy. Want to buy a used parser?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment\n",
    "# u'Hey, buddy. Want to buy a used parser'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是当它出现在HTML文档中时, `Comment` 对象会使用特殊的格式输出:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<b>\n",
      " <!--Hey, buddy. Want to buy a used parser?-->\n",
      "</b>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.b.prettify())\n",
    "# <b>\n",
    "#  <!--Hey, buddy. Want to buy a used parser?-->\n",
    "# </b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For HTML documents 对于 HTML 文档 ¶\n",
    "\n",
    "Beautiful Soup 定义了一些 NavigableString 子类来包含在特定 HTML 标记内找到的字符串。通过忽略可能代表页面内找到的编程指令的字符串，可以更轻松地找出页面的主体。 （这些类是 Beautiful Soup 4.9.0 中的新类，html5lib 解析器不使用它们。）\n",
    "\n",
    "### class bs4.Stylesheet bs4.样式表类 ¶\n",
    "\n",
    "代表嵌入式 CSS 样式表的 NavigableString 子类；也就是说，在文档解析期间在 `<style>` 标记内找到的任何字符串。\n",
    "\n",
    "### class bs4.Script 类 bs4.Script ¶\n",
    "\n",
    "代表嵌入式 Javascript 的 NavigableString 子类；也就是说，在文档解析期间在 `<script>` 标记内找到的任何字符串。\n",
    "\n",
    "### class bs4.Template bs4. 模板类 ¶\n",
    "代表嵌入式 HTML 模板的 NavigableString 子类；也就是说，在文档解析期间在 `<template>` 标记内找到的任何字符串。\n",
    "\n",
    "## For XML documents 对于 XML 文档 ¶\n",
    "\n",
    "Beautiful Soup 定义了一些 NavigableString 类来保存可在 XML 文档中找到的特殊类型的字符串。与 Comment 一样，这些类是 NavigableString 的子类，它们在输出时向字符串添加额外的内容。\n",
    "\n",
    "### class bs4.Declaration 类 bs4.声明 ¶\n",
    "\n",
    "NavigableString 子类，表示 XML 文档开头的声明。\n",
    "\n",
    "### class bs4.Doctype bs4.Doctype 类 ¶\n",
    "\n",
    "表示文档类型声明的 NavigableString 子类，可以在 XML 文档的开头附近找到。\n",
    "\n",
    "### class bs4.CData bs4.CData 类 ¶\n",
    "\n",
    "代表 CData 部分的 NavigableString 子类。\n",
    "\n",
    "### class bs4.ProcessingInstruction\n",
    "\n",
    "表示 XML 处理指令内容的 NavigableString 子类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 遍历文档树\n",
    "\n",
    "还拿”爱丽丝梦游仙境”的文档来做例子:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = \"\"\"<html><head><title>The Dormouse's story</title></head>\n",
    "    <body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过这段例子来演示怎样从文档的一段内容找到另一段内容\n",
    "\n",
    "## 子节点\n",
    "\n",
    "一个Tag可能包含多个字符串或其它的Tag,这些都是这个Tag的子节点.Beautiful Soup提供了许多操作和遍历子节点的属性.\n",
    "\n",
    "注意: Beautiful Soup中字符串节点不支持这些属性,因为字符串没有子节点\n",
    "\n",
    "### tag的名字\n",
    "\n",
    "操作文档树最简单的方法就是告诉它你想获取的tag的name. 如果想获取 `<head>` 标签,只要用 `soup.head` :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<head><title>The Dormouse's story</title></head>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<title>The Dormouse's story</title>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.head\n",
    "# <head><title>The Dormouse's story</title></head>\n",
    "\n",
    "soup.title\n",
    "# <title>The Dormouse's story</title>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是个获取tag的小窍门,可以在文档树的tag中多次调用这个方法.下面的代码可以获取<body>标签中的第一个<b>标签:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<b>The Dormouse's story</b>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.body.b\n",
    "# <b>The Dormouse's story</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用标签名称作为属性只会为您提供该名称的第一个标签："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.a\n",
    "# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想要得到所有的`<a>`标签,或是通过名字得到比一个tag更多的内容的时候,就需要用到 Searching the tree 中描述的方法,比如: find_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('a')\n",
    "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .contents 和 .children\n",
    "\n",
    "tag的 `.contents` 属性可以将tag的子节点以 `列表` `list`的方式输出:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<head><title>The Dormouse's story</title></head>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<title>The Dormouse's story</title>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<title>The Dormouse's story</title>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[\"The Dormouse's story\"]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_tag = soup.head\n",
    "head_tag\n",
    "# <head><title>The Dormouse's story</title></head>\n",
    "\n",
    "head_tag.contents\n",
    "# [<title>The Dormouse's story</title>]\n",
    "\n",
    "title_tag = head_tag.contents[0]\n",
    "title_tag\n",
    "# <title>The Dormouse's story</title>\n",
    "title_tag.contents\n",
    "# [u'The Dormouse's story']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BeautifulSoup` 对象本身一定会包含子节点,也就是说<html>标签也是 `BeautifulSoup` 对象的子节点:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'html'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(soup.contents)\n",
    "# 1\n",
    "soup.contents[0].name\n",
    "# u'html'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "字符串没有 `.contents` 属性,因为字符串没有子节点:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.NavigableString"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = title_tag.contents[0]\n",
    "# text.contents\n",
    "# AttributeError: 'NavigableString' object has no attribute 'contents'\n",
    "type(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过tag的 `.children` 生成器,可以对tag的子节点进行循环:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dormouse's story\n"
     ]
    }
   ],
   "source": [
    "for child in title_tag.children:\n",
    "    print(child)\n",
    "    # The Dormouse's story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .descendants 所有的后代\n",
    "\n",
    "`.contents` 和 `.children` 属性仅包含tag的`直接`子节点.例如,<head>标签只有一个直接子节点<title>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title>The Dormouse's story</title>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<list_iterator at 0x21530633130>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_tag.contents\n",
    "# [<title>The Dormouse's story</title>]\n",
    "head_tag.children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是<title>标签也包含一个子节点:字符串 “The Dormouse’s story”,这种情况下字符串 “The Dormouse’s story”也属于<head>标签的子孙节点. `.descendants` 属性可以对所有tag的子孙节点进行递归循环 [[5\\]](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id93) :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>The Dormouse's story</title>\n",
      "The Dormouse's story\n"
     ]
    }
   ],
   "source": [
    "for child in head_tag.descendants:\n",
    "    print(child)\n",
    "    # <title>The Dormouse's story</title>\n",
    "    # The Dormouse's story\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的例子中, <head>标签只有一个子节点,但是有2个子孙节点:<head>节点和<head>的子节点, `BeautifulSoup` 有一个直接子节点(<html>节点),却有很多子孙节点:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(soup.children))\n",
    "# 1\n",
    "len(list(soup.descendants))\n",
    "# 25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .string\n",
    "\n",
    "如果tag只有一个 `NavigableString` 类型子节点,那么这个tag可以使用 `.string` 得到子节点:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Dormouse's story\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_tag.string\n",
    "# u'The Dormouse's story'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果一个tag仅有一个子节点,那么这个tag也可以使用 `.string` 方法,输出结果与当前唯一子节点的 `.string` 结果相同:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title>The Dormouse's story</title>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"The Dormouse's story\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_tag.contents\n",
    "# [<title>The Dormouse's story</title>]\n",
    "\n",
    "head_tag.string\n",
    "# u'The Dormouse's story'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果tag包含了多个子节点,tag就无法确定 `.string` 方法应该调用哪个子节点的内容, `.string` 的输出结果是 `None` :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(soup.html.string)\n",
    "# None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .strings 和 stripped_strings\n",
    "\n",
    "如果tag中包含多个字符串 [[2\\]](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id90) ,可以使用 `.strings` 来循环获取:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The Dormouse's story\"\n",
      "'\\n'\n",
      "'\\n'\n",
      "\"The Dormouse's story\"\n",
      "'\\n'\n",
      "'Once upon a time there were three little sisters; and their names were\\n'\n",
      "'Elsie'\n",
      "',\\n'\n",
      "'Lacie'\n",
      "' and\\n'\n",
      "'Tillie'\n",
      "';\\nand they lived at the bottom of a well.'\n",
      "'\\n'\n",
      "'...'\n"
     ]
    }
   ],
   "source": [
    "for string in soup.strings:\n",
    "    print(repr(string))\n",
    "    # u\"The Dormouse's story\"\n",
    "    # u'\\n\\n'\n",
    "    # u\"The Dormouse's story\"\n",
    "    # u'\\n\\n'\n",
    "    # u'Once upon a time there were three little sisters; and their names were\\n'\n",
    "    # u'Elsie'\n",
    "    # u',\\n'\n",
    "    # u'Lacie'\n",
    "    # u' and\\n'\n",
    "    # u'Tillie'\n",
    "    # u';\\nand they lived at the bottom of a well.'\n",
    "    # u'\\n\\n'\n",
    "    # u'...'\n",
    "    # u'\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出的字符串中可能包含了很多空格或空行,使用 `.stripped_strings` 可以去除多余空白内容:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The Dormouse's story\"\n",
      "\"The Dormouse's story\"\n",
      "'Once upon a time there were three little sisters; and their names were'\n",
      "'Elsie'\n",
      "','\n",
      "'Lacie'\n",
      "'and'\n",
      "'Tillie'\n",
      "';\\nand they lived at the bottom of a well.'\n",
      "'...'\n"
     ]
    }
   ],
   "source": [
    "for string in soup.stripped_strings:\n",
    "    print(repr(string))\n",
    "    # u\"The Dormouse's story\"\n",
    "    # u\"The Dormouse's story\"\n",
    "    # u'Once upon a time there were three little sisters; and their names were'\n",
    "    # u'Elsie'\n",
    "    # u','\n",
    "    # u'Lacie'\n",
    "    # u'and'\n",
    "    # u'Tillie'\n",
    "    # u';\\nand they lived at the bottom of a well.'\n",
    "    # u'...'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全部是空格的行会被忽略掉,段首和段末的空白会被删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dormouse's story \n",
      " \n",
      " The Dormouse's story \n",
      " Once upon a time there were three little sisters; and their names were\n",
      " Elsie ,\n",
      " Lacie  and\n",
      " Tillie ;\n",
      "and they lived at the bottom of a well. \n",
      " ...\n",
      "stripped_strings 去除多余空白内容\n",
      "The Dormouse's story The Dormouse's story Once upon a time there were three little sisters; and their names were Elsie , Lacie and Tillie ;\n",
      "and they lived at the bottom of a well. ...\n"
     ]
    }
   ],
   "source": [
    "# 'generator' object has no attribute 'join'\n",
    "# Convert the generator object to a list\n",
    "print(' '.join(list(soup.strings)))\n",
    "print('stripped_strings 去除多余空白内容')\n",
    "print(' '.join(list(soup.stripped_strings)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 父节点\n",
    "\n",
    "继续分析文档树,每个tag或字符串都有父节点:被包含在某个tag中\n",
    "\n",
    "### .parent\n",
    "\n",
    "通过 `.parent` 属性来获取某个元素的父节点.在例子“爱丽丝”的文档中,<head>标签是<title>标签的父节点:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>The Dormouse's story</title>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<head><title>The Dormouse's story</title></head>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_tag = soup.title\n",
    "title_tag\n",
    "# <title>The Dormouse's story</title>\n",
    "title_tag.parent\n",
    "# <head><title>The Dormouse's story</title></head>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文档title的字符串也有父节点:<title>标签\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Dormouse's story\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<title>The Dormouse's story</title>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_tag.string\n",
    "title_tag.string.parent\n",
    "\n",
    "# <title>The Dormouse's story</title>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文档的顶层节点比如<html>的父节点是 `BeautifulSoup` 对象:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_tag = soup.html\n",
    "type(html_tag.parent)\n",
    "# <class 'bs4.BeautifulSoup'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BeautifulSoup` 对象的 `.parent` 是None:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(soup.parent)\n",
    "# None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .parents\n",
    "\n",
    "通过元素的 `.parents` 属性可以递归得到元素的所有父辈节点,下面的例子使用了 `.parents` 方法遍历了<a>标签到根节点的所有节点.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p\n",
      "body\n",
      "html\n",
      "[document]\n"
     ]
    }
   ],
   "source": [
    "link = soup.a\n",
    "link\n",
    "# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
    "for parent in link.parents:\n",
    "    if parent is None:\n",
    "        print(parent)\n",
    "    else:\n",
    "        print(parent.name)\n",
    "# p\n",
    "# body\n",
    "# html\n",
    "# [document]\n",
    "# None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 兄弟节点\n",
    "\n",
    "看一段简单的例子:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <body>\n",
      "  <a>\n",
      "   <b>\n",
      "    text1\n",
      "   </b>\n",
      "   <c>\n",
      "    text2\n",
      "   </c>\n",
      "  </a>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sibling_soup = BeautifulSoup(\"<a><b>text1</b><c>text2</c></b></a>\")\n",
    "print(sibling_soup.prettify())\n",
    "# <html>\n",
    "#  <body>\n",
    "#   <a>\n",
    "#    <b>\n",
    "#     text1\n",
    "#    </b>\n",
    "#    <c>\n",
    "#     text2\n",
    "#    </c>\n",
    "#   </a>\n",
    "#  </body>\n",
    "# </html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为<b>标签和<c>标签是同一层:他们是同一个元素的子节点,所以<b>和<c>可以被称为兄弟节点.一段文档以标准格式输出时,兄弟节点有相同的缩进级别.在代码中也可以使用这种关系.\n",
    "\n",
    "### .next_sibling 和 .previous_sibling\n",
    "\n",
    "在文档树中,使用 `.next_sibling` 和 `.previous_sibling` 属性来查询兄弟节点:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<b>text1</b>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<c>text2</c>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<b>text1</b>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sibling_soup.b\n",
    "\n",
    "sibling_soup.b.next_sibling\n",
    "# <c>text2</c>\n",
    "\n",
    "sibling_soup.c.previous_sibling\n",
    "# <b>text1</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<b>`标签有 `.next_sibling` 属性,但是没有 `.previous_sibling` 属性,因为<b>标签在同级节点中是第一个.同理,`<c>`标签有 `.previous_sibling` 属性,却没有 `.next_sibling` 属性:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(sibling_soup.b.previous_sibling)\n",
    "# None\n",
    "print(sibling_soup.c.next_sibling)\n",
    "# None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例子中的字符串“text1”和“text2”不是兄弟节点,因为它们的父节点不同(“text1”是b元素的子节点,而“text2”是C元素的子节点):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text1'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "sibling_soup.b.string\n",
    "# u'text1'\n",
    "\n",
    "print(sibling_soup.b.string.next_sibling)\n",
    "# None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际文档中的tag的 `.next_sibling` 和 `.previous_sibling` 属性通常是字符串或空白. 看看“爱丽丝”文档:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```html\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a>\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果以为第一个`<a>`标签的 `.next_sibling` 结果是第二个`<a>`标签,那就错了,真实结果是第一个`<a>`标签和第二个`<a>`标签之间的顿号和换行符:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "',\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link = soup.a\n",
    "link\n",
    "# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
    "\n",
    "link.next_sibling\n",
    "# u',\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个`<a>`标签是顿号的 `.next_sibling` 属性:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link.next_sibling.next_sibling\n",
    "# <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .next_siblings 和 .previous_siblings\n",
    "\n",
    "通过 `.next_siblings` 和 `.previous_siblings` 属性可以对当前节点的兄弟节点迭代输出:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "',\\n'\n",
      "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>\n",
      "' and\\n'\n",
      "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\n",
      "';\\nand they lived at the bottom of a well.'\n"
     ]
    }
   ],
   "source": [
    "for sibling in soup.a.next_siblings:\n",
    "    print(repr(sibling))\n",
    "    # u',\\n'\n",
    "    # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>\n",
    "    # u' and\\n'\n",
    "    # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\n",
    "    # u'; and they lived at the bottom of a well.'\n",
    "    # None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' and\\n'\n",
      "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>\n",
      "',\\n'\n",
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
      "'Once upon a time there were three little sisters; and their names were\\n'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for sibling in soup.find(id=\"link3\").previous_siblings:\n",
    "    print(repr(sibling))\n",
    "    # ' and\\n'\n",
    "    # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>\n",
    "    # u',\\n'\n",
    "    # <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
    "    # u'Once upon a time there were three little sisters; and their names were\\n'\n",
    "    # None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回退和前进\n",
    "\n",
    "看一下“爱丽丝” 文档:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```html\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML解析器把这段字符串转换成一连串的事件: “打开`<html>`标签”,”打开一个`<head>`标签”,”打开一个`<title>`标签”,”添加一段字符串”,”关闭`<title>`标签”,”关闭`<head>`标签””打开`<p>`标签”,等等.Beautiful Soup提供了重现解析器初始化过程的方法.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### .next_element 和 .previous_element\n",
    "\n",
    "`.next_element` 属性指向解析过程中下一个被解析的对象(字符串或tag),结果可能与 `.next_sibling` 相同,但通常是不一样的.\n",
    "\n",
    "这是“爱丽丝”文档中最后一个`<a>`标签,它的 `.next_sibling` 结果是一个字符串,因为当前的解析过程 [[2\\]](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id90) 因为当前的解析过程因为遇到了`<a>`标签而中断了:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "';\\nand they lived at the bottom of a well.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_a_tag = soup.find(\"a\", id=\"link3\")\n",
    "last_a_tag\n",
    "# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\n",
    "\n",
    "last_a_tag.next_sibling\n",
    "# '; and they lived at the bottom of a well.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但这个`<a>`标签的 `.next_element` 属性结果是在`<a>`标签被解析之后的解析内容,不是`<a>`标签后的句子部分,应该是字符串”Tillie”:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tillie'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_a_tag.next_element\n",
    "# u'Tillie'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是因为在原始文档中,字符串“Tillie” 在分号前出现,解析器先进入`<a>`标签,然后是字符串“Tillie”,然后关闭`</a>`标签,然后是分号和剩余部分.分号与`<a>`标签在同一层级,但是字符串“Tillie”会被先解析.\n",
    "\n",
    "`.previous_element` 属性刚好与 `.next_element` 相反,它指向当前被解析的对象的前一个解析对象:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' and\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_a_tag.previous_element\n",
    "# u' and\\n'\n",
    "last_a_tag.previous_element.next_element\n",
    "# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .next_elements 和 .previous_elements\n",
    "\n",
    "通过 `.next_elements` 和 `.previous_elements` 的迭代器就可以向前或向后访问文档的解析内容,就好像文档正在被解析一样:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Tillie'\n",
      "';\\nand they lived at the bottom of a well.'\n",
      "'\\n'\n",
      "<p class=\"story\">...</p>\n",
      "'...'\n"
     ]
    }
   ],
   "source": [
    "for element in last_a_tag.next_elements:\n",
    "    print(repr(element))\n",
    "# 'Tillie'\n",
    "# ';\\nand they lived at the bottom of a well.'\n",
    "# '\\n'\n",
    "# <p class=\"story\">...</p>\n",
    "# '...'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搜索文档树\n",
    "\n",
    "Beautiful Soup定义了很多搜索方法,这里着重介绍2个: `find()` 和 `find_all()` .其它方法的参数和用法类似,请读者举一反三.\n",
    "\n",
    "再以“爱丽丝”文档作为例子:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = \"\"\"<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 `find_all()` 类似的方法可以查找到想要查找的文档内容\n",
    "\n",
    "## 过滤器\n",
    "\n",
    "介绍 `find_all()` 方法前,先介绍一下过滤器的类型 [[3\\]](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id91) ,这些过滤器贯穿整个搜索的API.过滤器可以被用在tag的name中,节点的属性中,字符串中或他们的混合中.\n",
    "\n",
    "### 字符串\n",
    "\n",
    "最简单的过滤器是字符串.在搜索方法中传入一个字符串参数,Beautiful Soup会查找与字符串完整匹配的内容,下面的例子用于查找文档中所有的<b>标签:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<b>The Dormouse's story</b>]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('b')\n",
    "# [<b>The Dormouse's story</b>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果传入字节码参数,Beautiful Soup会当作UTF-8编码,可以传入一段Unicode 编码来避免Beautiful Soup解析编码出错\n",
    "\n",
    "### 正则表达式\n",
    "\n",
    "如果传入正则表达式作为参数,Beautiful Soup会通过正则表达式的 `search()` 来匹配内容.下面例子中找出所有以字母b开头的标签,这表示<body>和<b>标签都应该被找到:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "for tag in soup.find_all(re.compile(\"^b\")):\n",
    "    print(tag.name)\n",
    "# body\n",
    "# b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面代码找出所有名字中包含”t”的标签:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "html\n",
      "title\n"
     ]
    }
   ],
   "source": [
    "for tag in soup.find_all(re.compile(\"t\")):\n",
    "    print(tag.name)\n",
    "# html\n",
    "# title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 列表\n",
    "\n",
    "如果传入列表参数,Beautiful Soup会将与列表中任一元素匹配的内容返回.下面代码找到文档中所有`<a>`标签和`<b>`标签:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<b>The Dormouse's story</b>,\n",
       " <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all([\"a\", \"b\"])\n",
    "# [<b>The Dormouse's story</b>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True\n",
    "\n",
    "`True` 可以匹配任何值,下面代码查找到所有的tag,但是不会返回字符串节点\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "html\n",
      "head\n",
      "title\n",
      "body\n",
      "p\n",
      "b\n",
      "p\n",
      "a\n",
      "a\n",
      "a\n",
      "p\n"
     ]
    }
   ],
   "source": [
    "for tag in soup.find_all(True):\n",
    "    print(tag.name)\n",
    "# html\n",
    "# head\n",
    "# title\n",
    "# body\n",
    "# p\n",
    "# b\n",
    "# p\n",
    "# a\n",
    "# a\n",
    "# a\n",
    "# p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法\n",
    "\n",
    "如果没有合适过滤器,那么还可以定义一个方法,方法只接受一个元素参数 [[4\\]](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id92) ,如果这个方法返回 `True` 表示当前元素匹配并且被找到,如果不是则反回 `False`\n",
    "\n",
    "下面方法校验了当前元素,如果包含 `class` 属性却不包含 `id` 属性,那么将返回 `True`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_class_but_no_id(tag):\n",
    "    return tag.has_attr('class') and not tag.has_attr('id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将这个方法作为参数传入 `find_all()` 方法,将得到所有<p>标签:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"title\"><b>The Dormouse's story</b></p>,\n",
       " <p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
       " <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\n",
       " and they lived at the bottom of a well.</p>,\n",
       " <p class=\"story\">...</p>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(has_class_but_no_id)\n",
    "# [<p class=\"title\"><b>The Dormouse's story</b></p>,\n",
    "#  <p class=\"story\">Once upon a time there were...</p>,\n",
    "#  <p class=\"story\">...</p>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "返回结果中只有`<p>`标签没有`<a>`标签,因为`<a>`标签还定义了”id”,没有返回`<html>`和`<head>`,因为`<html>`和`<head>`中没有定义”class”属性.\n",
    "\n",
    "通过一个方法来过滤一类标签属性的时候, 这个方法传入的参数是`要被过滤的属性的值, 而不是这个标签`. 下面的例子是找出 `href` 属性不符合指定正则的 `a` 标签.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "http://example.com/elsie\n",
      "http://example.com/lacie\n",
      "http://example.com/tillie\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def not_lacie(href):\n",
    "    print(href)\n",
    "    return href and not re.compile(\"lacie\").search(href)\n",
    "soup.find_all(href=not_lacie)\n",
    "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标签过滤方法可以使用复杂方法. 下面的例子可以过滤出前后都有文字的标签.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body\n",
      "p\n",
      "a\n",
      "a\n",
      "a\n",
      "p\n"
     ]
    }
   ],
   "source": [
    "from bs4 import NavigableString\n",
    "def surrounded_by_strings(tag):\n",
    "    return (isinstance(tag.next_element, NavigableString)\n",
    "            and isinstance(tag.previous_element, NavigableString))\n",
    "\n",
    "for tag in soup.find_all(surrounded_by_strings):\n",
    "    print(tag.name)\n",
    "# p\n",
    "# a\n",
    "# a\n",
    "# a\n",
    "# p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在来了解一下搜索方法的细节\n",
    "\n",
    "## find_all()\n",
    "\n",
    "find_all( [name](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id36) , [attrs](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#css) , [recursive](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#recursive) , [string](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id37) , [**kwargs](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#keyword) )\n",
    "\n",
    "`find_all()` 方法搜索当前tag的所有tag子节点,并判断是否符合过滤器的条件.这里有几个例子:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title>The Dormouse's story</title>]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<p class=\"title\"><b>The Dormouse's story</b></p>]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Once upon a time there were three little sisters; and their names were\\n'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"title\")\n",
    "# [<title>The Dormouse's story</title>]\n",
    "\n",
    "soup.find_all(\"p\", \"title\")\n",
    "# [<p class=\"title\"><b>The Dormouse's story</b></p>]\n",
    "\n",
    "soup.find_all(\"a\")\n",
    "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
    "\n",
    "soup.find_all(id=\"link2\")\n",
    "# [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]\n",
    "\n",
    "import re\n",
    "soup.find(string=re.compile(\"sisters\"))\n",
    "# u'Once upon a time there were three little sisters; and their names were\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有几个方法很相似,还有几个方法是新的,参数中的 `string` 和 `id` 是什么含义? 为什么 `find_all(\"p\", \"title\")` 返回的是CSS Class为”title”的<p>标签? 我们来仔细看一下 `find_all()` 的参数\n",
    "\n",
    "### name 参数\n",
    "\n",
    "`name` 参数可以查找所有名字为 `name` 的tag,字符串对象会被自动忽略掉.\n",
    "\n",
    "简单的用法如下:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title>The Dormouse's story</title>]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"title\")\n",
    "# [<title>The Dormouse's story</title>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重申: 搜索 `name` 参数的值可以使任一类型的 [过滤器](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id29) ,字符窜,正则表达式,列表,方法或是 `True` .\n",
    "\n",
    "### keyword 参数\n",
    "\n",
    "如果一个指定名字的参数不是搜索内置的参数名,搜索时会把该参数当作指定名字tag的属性来搜索,如果包含一个名字为 `id` 的参数,Beautiful Soup会搜索每个tag的”id”属性.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(id='link2')\n",
    "# [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果传入 `href` 参数,Beautiful Soup会搜索每个tag的”href”属性:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(href=re.compile(\"elsie\"))\n",
    "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "搜索指定名字的属性时可以使用的参数值包括 [字符串](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id31) , [正则表达式](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id32) , [列表](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id33), [True](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#true) .\n",
    "\n",
    "下面的例子在文档树中查找所有包含 `id` 属性的tag,无论 `id` 的值是什么:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(id=True)\n",
    "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用多个指定名字的参数可以同时过滤tag的多个属性:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(href=re.compile(\"elsie\"), id='link1')\n",
    "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">three</a>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有些tag属性在搜索不能使用,比如HTML5中的 data-* 属性:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_soup = BeautifulSoup('<div data-foo=\"value\">foo!</div>')\n",
    "# data_soup.find_all(data-foo=\"value\")\n",
    "# SyntaxError: expression cannot contain assignment, perhaps you meant \"==\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是可以通过 `find_all()` 方法的 `attrs` 参数定义一个字典参数来搜索包含特殊属性的tag:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div data-foo=\"value\">foo!</div>]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_soup.find_all(attrs={\"data-foo\": \"value\"})\n",
    "# [<div data-foo=\"value\">foo!</div>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class_ 参数 按CSS类名搜索\n",
    "\n",
    "按照CSS类名搜索tag的功能非常实用,但标识CSSc的关键字 `class` 在Python中是保留字,使用 `class` 做参数会导致语法错误.从Beautiful Soup的4.1.1版本开始,可以通过 `class_` 参数搜索有指定CSS类名的tag:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"a\", class_=\"sister\")\n",
    "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`class_` 参数同样接受不同类型的 `过滤器` ,字符串,正则表达式,方法或 `True` :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"title\"><b>The Dormouse's story</b></p>]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(class_=re.compile(\"itl\"))\n",
    "# [<p class=\"title\"><b>The Dormouse's story</b></p>]\n",
    "\n",
    "def has_six_characters(css_class):\n",
    "    return css_class is not None and len(css_class) == 6\n",
    "\n",
    "soup.find_all(class_=has_six_characters)\n",
    "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tag的 `class` 属性是 多值属性 .按照CSS类名搜索tag时,可以分别搜索tag中的每个CSS类名:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"body strikeout\"></p>]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<p class=\"body strikeout\"></p>]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "css_soup = BeautifulSoup('<p class=\"body strikeout\"></p>')\n",
    "css_soup.find_all(\"p\", class_=\"strikeout\")\n",
    "# [<p class=\"body strikeout\"></p>]\n",
    "\n",
    "css_soup.find_all(\"p\", class_=\"body\")\n",
    "# [<p class=\"body strikeout\"></p>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "搜索 `class` 属性时也可以通过CSS值完全匹配:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"body strikeout\"></p>]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "css_soup.find_all(\"p\", class_=\"body strikeout\")\n",
    "# [<p class=\"body strikeout\"></p>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完全匹配 `class` 的值时,如果CSS类名的顺序与实际不符,将搜索不到结果:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"a\", attrs={\"class\": \"sister\"})\n",
    "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `string` 参数\n",
    "\n",
    "通过 `string` 参数可以搜搜文档中的字符串内容.与 `name` 参数的可选值一样, `string` 参数接受 [字符串](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id31) , [正则表达式](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id32) , [列表](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id33), [True](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#true) . 看例子:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Elsie']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Elsie', 'Lacie', 'Tillie']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[\"The Dormouse's story\", \"The Dormouse's story\"]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[\"The Dormouse's story\", \"The Dormouse's story\"]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[\"The Dormouse's story\",\n",
       " \"The Dormouse's story\",\n",
       " 'Elsie',\n",
       " 'Lacie',\n",
       " 'Tillie',\n",
       " '...']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(string=\"Elsie\")\n",
    "# [u'Elsie']\n",
    "\n",
    "soup.find_all(string=[\"Tillie\", \"Elsie\", \"Lacie\"])\n",
    "# [u'Elsie', u'Lacie', u'Tillie']\n",
    "\n",
    "soup.find_all(string=re.compile(\"Dormouse\"))\n",
    "[u\"The Dormouse's story\", u\"The Dormouse's story\"]\n",
    "\n",
    "def is_the_only_string_within_a_tag(s):\n",
    "    \"\"\"Return True if this string is the only child of its parent tag.\"\"\"\n",
    "    return (s == s.parent.string)\n",
    "\n",
    "soup.find_all(string=is_the_only_string_within_a_tag)\n",
    "# [u\"The Dormouse's story\", u\"The Dormouse's story\", u'Elsie', u'Lacie', u'Tillie', u'...']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然 `string` 参数用于搜索字符串,还可以与其它参数混合使用来过滤tag.Beautiful Soup会找到 `.string` 方法与 `string` 参数值相符的tag.下面代码用来搜索内容里面包含“Elsie”的<a>标签:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"a\", string=\"Elsie\")\n",
    "# [<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `limit` 参数\n",
    "\n",
    "`find_all()` 方法返回全部的搜索结构,如果文档树很大那么搜索会很慢.如果我们不需要全部结果,可以使用 `limit` 参数限制返回结果的数量.效果与SQL中的limit关键字类似,当搜索到的结果数量达到 `limit` 的限制时,就停止搜索返回结果.\n",
    "\n",
    "文档树中有3个tag符合搜索条件,但结果只返回了2个,因为我们限制了返回数量:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"a\", limit=2)\n",
    "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `recursive` 参数\n",
    "\n",
    "调用tag的 `find_all()` 方法时,Beautiful Soup会检索当前tag的所有子孙节点,如果只想搜索tag的直接子节点,可以使用参数 `recursive=False` .\n",
    "\n",
    "一段简单的文档:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```html\n",
    "<html>\n",
    " <head>\n",
    "  <title>\n",
    "   The Dormouse's story\n",
    "  </title>\n",
    " </head>\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "是否使用 `recursive` 参数的搜索结果:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title>The Dormouse's story</title>]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.html.find_all(\"title\")\n",
    "# [<title>The Dormouse's story</title>]\n",
    "\n",
    "soup.html.find_all(\"title\", recursive=False)\n",
    "# []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```html\n",
    "<title>标签在 <html> 标签下, 但并不是直接子节点, <head> 标签才是直接子节点. 在允许查询所有后代节点时 Beautiful Soup 能够查找到 <title> 标签. 但是使用了 recursive=False 参数之后,只能查找直接子节点,这样就查不到 <title> 标签了.\n",
    "\n",
    "Beautiful Soup 提供了多种DOM树搜索方法. 这些方法都使用了类似的参数定义. 比如这些方法: find_all(): name, attrs, text, limit. 但是只有 find_all() 和 find() 支持 recursive 参数.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 像调用 find_all() 一样调用tag\n",
    "find_all() 几乎是Beautiful Soup中最常用的搜索方法,所以我们定义了它的简写方法. \n",
    "\n",
    "BeautifulSoup 对象和 tag 对象可以`被当作一个方法来使用`,这个方法的执行结果与调用这个对象的 find_all() 方法相同,下面两行代码是等价的:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<title>标签在 <html> 标签下, 但并不是直接子节点, <head> 标签才是直接子节点. 在允许查询所有后代节点时 Beautiful Soup 能够查找到 <title> 标签. 但是使用了 `recursive=False` 参数之后,只能查找直接子节点,这样就查不到 <title> 标签了.\n",
    "\n",
    "Beautiful Soup 提供了多种DOM树搜索方法. 这些方法都使用了类似的参数定义. 比如这些方法: `find_all()`: `name`, `attrs`, `text`, `limit`. 但是只有 `find_all()` 和 `find()` 支持 `recursive` 参数.\n",
    "\n",
    "## 像调用 `find_all()` 一样调用tag\n",
    "\n",
    "`find_all()` 几乎是Beautiful Soup中最常用的搜索方法,所以我们定义了它的简写方法. `BeautifulSoup` 对象和 `tag` 对象可以被当作一个方法来使用,这个方法的执行结果与调用这个对象的 `find_all()` 方法相同,下面两行代码是等价的:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"a\")\n",
    "soup(\"a\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这两行代码也是等价的:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The Dormouse's story\"]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[\"The Dormouse's story\"]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.find_all(string=True)\n",
    "soup.title(string=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find()\n",
    "\n",
    "find( [name](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id36) , [attrs](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#css) , [recursive](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#recursive) , [string](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id37) , [**kwargs](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#keyword) )\n",
    "\n",
    "`find_all()` 方法将返回文档中符合条件的所有tag,尽管有时候我们只想得到一个结果.比如文档中只有一个<body>标签,那么使用 `find_all()` 方法来查找<body>标签就不太合适, \n",
    "\n",
    "使用 `find_all` 方法并设置 `limit=1` 参数不如直接使用 `find()` 方法.下面两行代码是等价的:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title>The Dormouse's story</title>]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<title>The Dormouse's story</title>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('title', limit=1)\n",
    "# [<title>The Dormouse's story</title>]\n",
    "\n",
    "soup.find('title')\n",
    "# <title>The Dormouse's story</title>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "唯一的区别是 `find_all()` 方法的返回结果是值包含一个元素的列表,而 `find()` 方法直接返回结果.\n",
    "\n",
    "`find_all()` 方法没有找到目标是返回空列表, `find()` 方法找不到目标时,返回 `None` .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(soup.find(\"nosuchtag\"))\n",
    "# None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`soup.head.title` 是 `tag的名字 方法的简写`.这个简写的原理就是多次调用当前tag的 `find()` 方法:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>The Dormouse's story</title>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<title>The Dormouse's story</title>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.head.title\n",
    "# <title>The Dormouse's story</title>\n",
    "\n",
    "soup.find(\"head\").find(\"title\")\n",
    "# <title>The Dormouse's story</title>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find_parents() 和 find_parent()\n",
    "\n",
    "find_parents( [name](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id36) , [attrs](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#css) , [recursive](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#recursive) , [string](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id37) , [**kwargs](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#keyword) )\n",
    "\n",
    "find_parent( [name](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id36) , [attrs](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#css) , [recursive](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#recursive) , [string](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id37) , [**kwargs](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#keyword) )\n",
    "\n",
    "我们已经用了很大篇幅来介绍 `find_all()` 和 `find()` 方法,Beautiful Soup中还有10个用于搜索的API.它们中的五个用的是与 `find_all()` 相同的搜索参数,另外5个与 `find()` 方法的搜索参数类似.区别仅是它们搜索文档的不同部分.\n",
    "\n",
    "记住: `find_all()` 和 `find()` 只搜索当前节点的所有子节点,孙子节点等. `find_parents()` 和 `find_parent()` 用来搜索当前节点的父辈节点,搜索方法与普通tag的搜索方法相同,搜索文档搜索文档包含的内容. 我们从一个文档中的一个叶子节点开始:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lacie'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
       "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\n",
       "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\n",
       "and they lived at the bottom of a well.</p>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_string = soup.find(string=\"Lacie\")\n",
    "a_string\n",
    "# u'Lacie'\n",
    "\n",
    "a_string.find_parents(\"a\")\n",
    "# [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]\n",
    "\n",
    "a_string.find_parent(\"p\")\n",
    "# <p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "#  <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\n",
    "#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\n",
    "#  and they lived at the bottom of a well.</p>\n",
    "\n",
    "a_string.find_parents(\"p\", class_=\"title\")\n",
    "# []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "文档中的一个`<a>`标签是是当前叶子节点的直接父节点,所以可以被找到.还有一个`<p>`标签,是目标叶子节点的间接父辈节点,所以也可以被找到.包含class值为”title”的`<p>`标签不是不是目标叶子节点的父辈节点,所以通过 `find_parents()` 方法搜索不到.\n",
    "\n",
    "`find_parent()` 和 `find_parents()` 方法会让人联想到 [.parent](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#parent) 和 [.parents](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#parents) 属性.它们之间的联系非常紧密.搜索父辈节点的方法实际上就是对 `.parents` 属性的迭代搜索.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## find_next_siblings() 和 find_next_sibling()\n",
    "\n",
    "find_next_siblings( [name](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id36) , [attrs](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#css) , [recursive](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#recursive) , [string](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id37) , [**kwargs](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#keyword) )\n",
    "\n",
    "find_next_sibling( [name](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id36) , [attrs](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#css) , [recursive](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#recursive) , [string](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id37) , [**kwargs](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#keyword) )\n",
    "\n",
    "这2个方法通过 [.next_siblings](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#next-siblings-previous-siblings) 属性对当tag的所有后面解析 [[5\\]](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id93) 的兄弟tag节点进行迭代, `find_next_siblings()` 方法返回所有符合条件的后面的兄弟节点, `find_next_sibling()` 只返回符合条件的后面的第一个tag节点.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<p class=\"story\">...</p>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_link = soup.a\n",
    "first_link\n",
    "# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
    "\n",
    "first_link.find_next_siblings(\"a\")\n",
    "# [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
    "\n",
    "first_story_paragraph = soup.find(\"p\", \"story\")\n",
    "first_story_paragraph.find_next_sibling(\"p\")\n",
    "# <p class=\"story\">...</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find_previous_siblings() 和 find_previous_sibling()\n",
    "\n",
    "find_previous_siblings( [name](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id36) , [attrs](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#css) , [recursive](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#recursive) , [string](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id37) , [**kwargs](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#keyword) )\n",
    "\n",
    "find_previous_sibling( [name](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id36) , [attrs](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#css) , [recursive](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#recursive) , [string](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id37) , [**kwargs](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#keyword) )\n",
    "\n",
    "这2个方法通过 [.previous_siblings](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#next-siblings-previous-siblings) 属性对当前tag的前面解析 [[5\\]](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id93) 的兄弟tag节点进行迭代, `find_previous_siblings()` 方法返回所有符合条件的前面的兄弟节点, `find_previous_sibling()` 方法返回第一个符合条件的前面的兄弟节点:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<p class=\"title\"><b>The Dormouse's story</b></p>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_link = soup.find(\"a\", id=\"link3\")\n",
    "last_link\n",
    "# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\n",
    "\n",
    "last_link.find_previous_siblings(\"a\")\n",
    "# [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>]\n",
    "\n",
    "first_story_paragraph = soup.find(\"p\", \"story\")\n",
    "first_story_paragraph.find_previous_sibling(\"p\")\n",
    "# <p class=\"title\"><b>The Dormouse's story</b></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find_all_next() 和 find_next()\n",
    "\n",
    "find_all_next( [name](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id36) , [attrs](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#css) , [recursive](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#recursive) , [string](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id37) , [**kwargs](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#keyword) )\n",
    "\n",
    "find_next( [name](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id36) , [attrs](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#css) , [recursive](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#recursive) , [string](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id37) , [**kwargs](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#keyword) )\n",
    "\n",
    "这2个方法通过 [.next_elements](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#next-elements-previous-elements) 属性对当前tag的之后的 [[5\\]](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id93) tag和字符串进行迭代, `find_all_next()` 方法返回所有符合条件的节点, `find_next()` 方法返回第一个符合条件的节点:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Elsie',\n",
       " ',\\n',\n",
       " 'Lacie',\n",
       " ' and\\n',\n",
       " 'Tillie',\n",
       " ';\\nand they lived at the bottom of a well.',\n",
       " '\\n',\n",
       " '...']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<p class=\"story\">...</p>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_link = soup.a\n",
    "first_link\n",
    "# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
    "\n",
    "first_link.find_all_next(string=True)\n",
    "# [u'Elsie', u',\\n', u'Lacie', u' and\\n', u'Tillie',\n",
    "#  u';\\nand they lived at the bottom of a well.', u'\\n\\n', u'...', u'\\n']\n",
    "\n",
    "first_link.find_next(\"p\")\n",
    "# <p class=\"story\">...</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个例子中,字符串 “Elsie”也被显示出来,尽管它被包含在我们开始查找的`<a>`标签的里面.第二个例子中,最后一个`<p>`标签也被显示出来,尽管它与我们开始查找位置的`<a>`标签不属于同一部分.例子中,搜索的重点是要匹配过滤器的条件,并且在文档中出现的顺序而不是开始查找的元素的位置.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## find_all_previous() 和 find_previous()\n",
    "\n",
    "find_all_previous( [name](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id36) , [attrs](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#css) , [recursive](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#recursive) , [string](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id37) , [**kwargs](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#keyword) )\n",
    "\n",
    "find_previous( [name](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id36) , [attrs](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#css) , [recursive](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#recursive) , [string](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id37) , [**kwargs](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#keyword) )\n",
    "\n",
    "这2个方法通过 [.previous_elements](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#next-elements-previous-elements) 属性对当前节点前面 [[5\\]](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id93) 的tag和字符串进行迭代, `find_all_previous()` 方法返回所有符合条件的节点, `find_previous()` 方法返回第一个符合条件的节点.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
       " <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\n",
       " and they lived at the bottom of a well.</p>,\n",
       " <p class=\"title\"><b>The Dormouse's story</b></p>]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<title>The Dormouse's story</title>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_link = soup.a\n",
    "first_link\n",
    "# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
    "\n",
    "first_link.find_all_previous(\"p\")\n",
    "# [<p class=\"story\">Once upon a time there were three little sisters; ...</p>,\n",
    "#  <p class=\"title\"><b>The Dormouse's story</b></p>]\n",
    "\n",
    "first_link.find_previous(\"title\")\n",
    "# <title>The Dormouse's story</title>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`find_all_previous(\"p\")` 返回了文档中的第一段(class=”title”的那段),但还返回了第二段,`<p>`标签包含了我们开始查找的`<a>`标签.不要惊讶,这段代码的功能是查找所有出现在指定`<a>`标签之前的`<p>`标签,因为这个`<p>`标签包含了开始的`<a>`标签,所以`<p>`标签一定是在`<a>`之前出现的.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## CSS选择器 select()\n",
    "\n",
    "Beautiful Soup支持大部分的CSS选择器 http://www.w3.org/TR/CSS2/selector.html [[6\\]](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id94) , \n",
    "在 `Tag` 或 `BeautifulSoup` 对象的 `.select()` 方法中传入字符串参数, 即可使用CSS选择器的语法找到tag:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title>The Dormouse's story</title>]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<p class=\"story\">...</p>]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select(\"title\")\n",
    "# [<title>The Dormouse's story</title>]\n",
    "\n",
    "soup.select(\"p:nth-of-type(3)\")\n",
    "# [<p class=\"story\">...</p>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过tag标签逐层查找:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<title>The Dormouse's story</title>]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select(\"body a\")\n",
    "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/lacie\"  id=\"link2\">Lacie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
    "\n",
    "soup.select(\"html head title\")\n",
    "# [<title>The Dormouse's story</title>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "找到某个tag标签下的直接子标签 [[6\\]](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id94) :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title>The Dormouse's story</title>]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select(\"head > title\")\n",
    "# [<title>The Dormouse's story</title>]\n",
    "\n",
    "soup.select(\"p > a\")\n",
    "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/lacie\"  id=\"link2\">Lacie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
    "\n",
    "soup.select(\"p > a:nth-of-type(2)\")\n",
    "# [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]\n",
    "\n",
    "soup.select(\"p > #link1\")\n",
    "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>]\n",
    "\n",
    "soup.select(\"body > a\")\n",
    "# []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "找到兄弟节点标签:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select(\"#link1 ~ .sister\")\n",
    "# [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/tillie\"  id=\"link3\">Tillie</a>]\n",
    "\n",
    "soup.select(\"#link1 + .sister\")\n",
    "# [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过CSS的类名查找:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select(\".sister\")\n",
    "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
    "\n",
    "soup.select(\"[class~=sister]\")\n",
    "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过tag的id查找:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select(\"#link1\")\n",
    "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>]\n",
    "\n",
    "soup.select(\"a#link2\")\n",
    "# [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时用多种CSS选择器查询元素:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select(\"#link1,#link2\")\n",
    "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过是否存在某个属性来查找:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('a[href]')\n",
    "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过属性的值来查找:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('a[href=\"http://example.com/elsie\"]')\n",
    "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>]\n",
    "\n",
    "soup.select('a[href^=\"http://example.com/\"]')\n",
    "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
    "\n",
    "soup.select('a[href$=\"tillie\"]')\n",
    "# [<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
    "\n",
    "soup.select('a[href*=\".com/el\"]')\n",
    "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过语言设置来查找:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p lang=\"en\">Hello</p>,\n",
       " <p lang=\"en-us\">Howdy, y'all</p>,\n",
       " <p lang=\"en-gb\">Pip-pip, old fruit</p>]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilingual_markup = \"\"\"\n",
    " <p lang=\"en\">Hello</p>\n",
    " <p lang=\"en-us\">Howdy, y'all</p>\n",
    " <p lang=\"en-gb\">Pip-pip, old fruit</p>\n",
    " <p lang=\"fr\">Bonjour mes amis</p>\n",
    "\"\"\"\n",
    "multilingual_soup = BeautifulSoup(multilingual_markup)\n",
    "multilingual_soup.select('p[lang|=en]')\n",
    "# [<p lang=\"en\">Hello</p>,\n",
    "#  <p lang=\"en-us\">Howdy, y'all</p>,\n",
    "#  <p lang=\"en-gb\">Pip-pip, old fruit</p>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "返回查找到的元素的第一个\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select_one(\".sister\")\n",
    "# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于熟悉CSS选择器语法的人来说这是个非常方便的方法.Beautiful Soup也支持CSS选择器API, 如果你仅仅需要CSS选择器的功能,那么直接使用 `lxml` 也可以, 而且速度更快,支持更多的CSS选择器语法,但Beautiful Soup整合了CSS选择器的语法和自身方便使用API.\n",
    "\n",
    "# 修改文档树\n",
    "\n",
    "Beautiful Soup的强项是文档树的搜索,但同时也可以方便的修改文档树\n",
    "\n",
    "## 修改tag的名称和属性\n",
    "\n",
    "在 [Attributes](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#attributes) 的章节中已经介绍过这个功能,但是再看一遍也无妨. 重命名一个tag,改变属性的值,添加或删除属性:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<blockquote class=\"verybold\" id=\"1\">Extremely bold</blockquote>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<blockquote>Extremely bold</blockquote>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup('<b class=\"boldest\">Extremely bold</b>')\n",
    "tag = soup.b\n",
    "\n",
    "tag.name = \"blockquote\"\n",
    "tag['class'] = 'verybold'\n",
    "tag['id'] = 1\n",
    "tag\n",
    "# <blockquote class=\"verybold\" id=\"1\">Extremely bold</blockquote>\n",
    "\n",
    "del tag['class']\n",
    "del tag['id']\n",
    "tag\n",
    "# <blockquote>Extremely bold</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 修改 .string\n",
    "\n",
    "给tag的 `.string` 属性赋值,就相当于用当前的内容替代了原来的内容:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"http://example.com/\">New link text.</a>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markup = '<a href=\"http://example.com/\">I linked to <i>example.com</i></a>'\n",
    "soup = BeautifulSoup(markup)\n",
    "\n",
    "tag = soup.a\n",
    "tag.string = \"New link text.\"\n",
    "tag\n",
    "# <a href=\"http://example.com/\">New link text.</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意: 如果当前的tag包含了其它tag,那么给它的 `.string` 属性赋值会覆盖掉原有的所有内容包括子tag\n",
    "\n",
    "## append()\n",
    "\n",
    "`Tag.append()` 方法想tag中添加内容,就好像Python的列表的 `.append()` 方法:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><body><a>FooBar</a></body></html>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Foo', 'Bar']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(\"<a>Foo</a>\")\n",
    "soup.a.append(\"Bar\")\n",
    "\n",
    "soup\n",
    "# <html><head></head><body><a>FooBar</a></body></html>\n",
    "soup.a.contents\n",
    "# [u'Foo', u'Bar']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NavigableString() 和 .new_tag()\n",
    "\n",
    "如果想添加一段文本内容到文档中也没问题,可以调用Python的 `append()` 方法 或调用 `NavigableString` 的构造方法:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<b>Hello there</b>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Hello', ' there']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(\"<b></b>\")\n",
    "tag = soup.b\n",
    "tag.append(\"Hello\")\n",
    "new_string = NavigableString(\" there\")\n",
    "tag.append(new_string)\n",
    "tag\n",
    "# <b>Hello there.</b>\n",
    "tag.contents\n",
    "# [u'Hello', u' there']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想要创建一段注释,或 `NavigableString` 的任何子类, 只要调用 NavigableString 的构造方法:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<b>Hello there<!--Nice to see you.--></b>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Hello', ' there', 'Nice to see you.']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import Comment\n",
    "new_comment = soup.new_string(\"Nice to see you.\", Comment)\n",
    "tag.append(new_comment)\n",
    "tag\n",
    "# <b>Hello there<!--Nice to see you.--></b>\n",
    "tag.contents\n",
    "# [u'Hello', u' there', u'Nice to see you.']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# 这是Beautiful Soup 4.2.1 中新增的方法\n",
    "\n",
    "创建一个tag最好的方法是调用工厂方法 `BeautifulSoup.new_tag()` :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<b><a href=\"http://www.example.com\"></a></b>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<b><a href=\"http://www.example.com\">Link text.</a></b>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(\"<b></b>\")\n",
    "original_tag = soup.b\n",
    "\n",
    "new_tag = soup.new_tag(\"a\", href=\"http://www.example.com\")\n",
    "original_tag.append(new_tag)\n",
    "original_tag\n",
    "# <b><a href=\"http://www.example.com\"></a></b>\n",
    "\n",
    "new_tag.string = \"Link text.\"\n",
    "original_tag\n",
    "# <b><a href=\"http://www.example.com\">Link text.</a></b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个参数作为tag的name,是必填,其它参数选填\n",
    "\n",
    "## insert()\n",
    "\n",
    "`Tag.insert()` 方法与 `Tag.append()` 方法类似,区别是不会把新元素添加到父节点 `.contents` 属性的最后,而是把元素插入到指定的位置.与Python列表总的 `.insert()` 方法的用法下同:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"http://example.com/\">I linked to but did not endorse <i>example.com</i></a>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['I linked to ', 'but did not endorse ', <i>example.com</i>]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markup = '<a href=\"http://example.com/\">I linked to <i>example.com</i></a>'\n",
    "soup = BeautifulSoup(markup)\n",
    "tag = soup.a\n",
    "\n",
    "tag.insert(1, \"but did not endorse \")\n",
    "tag\n",
    "# <a href=\"http://example.com/\">I linked to but did not endorse <i>example.com</i></a>\n",
    "tag.contents\n",
    "# [u'I linked to ', u'but did not endorse', <i>example.com</i>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## insert_before() 和 insert_after()\n",
    "\n",
    "`insert_before()` 方法在当前tag或文本节点前插入内容:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<b><i>Don't</i>stop</b>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(\"<b>stop</b>\")\n",
    "tag = soup.new_tag(\"i\")\n",
    "tag.string = \"Don't\"\n",
    "soup.b.string.insert_before(tag)\n",
    "soup.b\n",
    "# <b><i>Don't</i>stop</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`insert_after()` 方法在当前tag或文本节点后插入内容:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<b><i>Don't</i> ever stop</b>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<i>Don't</i>, ' ever ', 'stop']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.b.i.insert_after(soup.new_string(\" ever \"))\n",
    "soup.b\n",
    "# <b><i>Don't</i> ever stop</b>\n",
    "soup.b.contents\n",
    "# [<i>Don't</i>, u' ever ', u'stop']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clear()\n",
    "\n",
    "`Tag.clear()` 方法移除当前tag的内容:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"http://example.com/\"></a>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markup = '<a href=\"http://example.com/\">I linked to <i>example.com</i></a>'\n",
    "soup = BeautifulSoup(markup)\n",
    "tag = soup.a\n",
    "\n",
    "tag.clear()\n",
    "tag\n",
    "# <a href=\"http://example.com/\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract()\n",
    "\n",
    "`PageElement.extract()` 方法将当前tag移除文档树,并作为方法结果返回:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"http://example.com/\">I linked to </a>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<i>example.com</i>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "markup = '<a href=\"http://example.com/\">I linked to <i>example.com</i></a>'\n",
    "soup = BeautifulSoup(markup)\n",
    "a_tag = soup.a\n",
    "\n",
    "i_tag = soup.i.extract()\n",
    "\n",
    "a_tag\n",
    "# <a href=\"http://example.com/\">I linked to</a>\n",
    "\n",
    "i_tag\n",
    "# <i>example.com</i>\n",
    "\n",
    "print(i_tag.parent)\n",
    "None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个方法实际上产生了2个文档树: 一个是用来解析原始文档的 `BeautifulSoup` 对象,另一个是被移除并且返回的tag.被移除并返回的tag可以继续调用 `extract` 方法:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'example.com'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<i></i>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string = i_tag.string.extract()\n",
    "my_string\n",
    "# u'example.com'\n",
    "\n",
    "print(my_string.parent)\n",
    "# None\n",
    "i_tag\n",
    "# <i></i>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decompose()\n",
    "\n",
    "`Tag.decompose()` 方法将当前节点移除文档树并完全销毁:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"http://example.com/\">I linked to </a>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markup = '<a href=\"http://example.com/\">I linked to <i>example.com</i></a>'\n",
    "soup = BeautifulSoup(markup)\n",
    "a_tag = soup.a\n",
    "\n",
    "soup.i.decompose()\n",
    "\n",
    "a_tag\n",
    "# <a href=\"http://example.com/\">I linked to</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## replace_with()\n",
    "\n",
    "`PageElement.replace_with()` 方法移除文档树中的某段内容,并用新tag或文本节点替代它:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<i>example.com</i>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<a href=\"http://example.com/\">I linked to <b>example.net</b></a>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markup = '<a href=\"http://example.com/\">I linked to <i>example.com</i></a>'\n",
    "soup = BeautifulSoup(markup)\n",
    "a_tag = soup.a\n",
    "\n",
    "new_tag = soup.new_tag(\"b\")\n",
    "new_tag.string = \"example.net\"\n",
    "a_tag.i.replace_with(new_tag)\n",
    "\n",
    "a_tag\n",
    "# <a href=\"http://example.com/\">I linked to <b>example.net</b></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`replace_with()` 方法返回被替代的tag或文本节点,可以用来浏览或添加到文档树其它地方\n",
    "\n",
    "## wrap()\n",
    "\n",
    "`PageElement.wrap()` 方法可以对指定的tag元素进行包装 [[8\\]](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id96) ,并返回包装后的结果:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<b>I wish I was bold.</b>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<div><p><b>I wish I was bold.</b></p></div>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(\"<p>I wish I was bold.</p>\")\n",
    "soup.p.string.wrap(soup.new_tag(\"b\"))\n",
    "# <b>I wish I was bold.</b>\n",
    "\n",
    "soup.p.wrap(soup.new_tag(\"div\"))\n",
    "# <div><p><b>I wish I was bold.</b></p></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该方法在 Beautiful Soup 4.0.5 中添加\n",
    "\n",
    "## unwrap()\n",
    "\n",
    "`Tag.unwrap()` 方法与 `wrap()` 方法相反.将移除tag内的所有tag标签,该方法常被用来进行标记的解包:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<i></i>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<a href=\"http://example.com/\">I linked to example.com</a>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markup = '<a href=\"http://example.com/\">I linked to <i>example.com</i></a>'\n",
    "soup = BeautifulSoup(markup)\n",
    "a_tag = soup.a\n",
    "\n",
    "a_tag.i.unwrap()\n",
    "a_tag\n",
    "# <a href=\"http://example.com/\">I linked to example.com</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与 `replace_with()` 方法相同, `unwrap()` 方法返回被移除的tag\n",
    "\n",
    "# 输出\n",
    "\n",
    "## 格式化输出\n",
    "\n",
    "`prettify()` 方法将Beautiful Soup的文档树格式化后以Unicode编码输出,每个XML/HTML标签都独占一行\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html>\\n <body>\\n  <a href=\"http://example.com/\">\\n   I linked to\\n   <i>\\n    example.com\\n   </i>\\n  </a>\\n </body>\\n</html>\\n'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <body>\n",
      "  <a href=\"http://example.com/\">\n",
      "   I linked to\n",
      "   <i>\n",
      "    example.com\n",
      "   </i>\n",
      "  </a>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "markup = '<a href=\"http://example.com/\">I linked to <i>example.com</i></a>'\n",
    "soup = BeautifulSoup(markup)\n",
    "soup.prettify()\n",
    "# '<html>\\n <head>\\n </head>\\n <body>\\n  <a href=\"http://example.com/\">\\n...'\n",
    "\n",
    "print(soup.prettify())\n",
    "# <html>\n",
    "#  <head>\n",
    "#  </head>\n",
    "#  <body>\n",
    "#   <a href=\"http://example.com/\">\n",
    "#    I linked to\n",
    "#    <i>\n",
    "#     example.com\n",
    "#    </i>\n",
    "#   </a>\n",
    "#  </body>\n",
    "# </html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BeautifulSoup` 对象和它的tag节点都可以调用 `prettify()` 方法:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"http://example.com/\">\n",
      " I linked to\n",
      " <i>\n",
      "  example.com\n",
      " </i>\n",
      "</a>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.a.prettify())\n",
    "# <a href=\"http://example.com/\">\n",
    "#  I linked to\n",
    "#  <i>\n",
    "#   example.com\n",
    "#  </i>\n",
    "# </a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 压缩输出\n",
    "\n",
    "如果只想得到结果字符串,不重视格式,那么可以对一个 `BeautifulSoup` 对象或 `Tag` 对象使用Python的 `unicode()` 或 `str()` 方法:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html><body><a href=\"http://example.com/\">I linked to <i>example.com</i></a></body></html>'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'<a href=\"http://example.com/\">I linked to <i>example.com</i></a>'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(soup)\n",
    "# '<html><head></head><body><a href=\"http://example.com/\">I linked to <i>example.com</i></a></body></html>'\n",
    "\n",
    "str(soup.a)\n",
    "# u'<a href=\"http://example.com/\">I linked to <i>example.com</i></a>'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`str()` 方法返回UTF-8编码的字符串,可以指定 [编码](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id56) 的设置.\n",
    "\n",
    "PYTHON2 还可以调用 `encode()` 方法获得字节码或调用 `decode()` 方法获得Unicode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 输出格式\n",
    "\n",
    "Beautiful Soup输出是会将HTML中的特殊字符转换成Unicode,比如“&lquot;”:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html><body><p>“Dammit!” he said.</p></body></html>'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(\"&ldquo;Dammit!&rdquo; he said.\")\n",
    "str(soup)\n",
    "# u'<html><head></head><body>\\u201cDammit!\\u201d he said.</body></html>'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果将文档转换成字符串,Unicode编码会被编码成UTF-8.这样就无法正确显示HTML特殊字符了:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html><body><p>“Dammit!” he said.</p></body></html>'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(soup)\n",
    "# '<html><head></head><body>\\xe2\\x80\\x9cDammit!\\xe2\\x80\\x9d he said.</body></html>'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_text()\n",
    "\n",
    "如果只想得到tag中包含的文本内容,那么可以调用 `get_text()` 方法,这个方法获取到tag中包含的所有文版内容包括子孙tag中的内容,并将结果作为Unicode字符串返回:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nI linked to example.com\\n'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\nI linked to example.com\\n'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'example.com'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'example.com'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markup = '<a href=\"http://example.com/\">\\nI linked to <i>example.com</i>\\n</a>'\n",
    "soup = BeautifulSoup(markup)\n",
    "\n",
    "soup.get_text()\n",
    "u'\\nI linked to example.com\\n'\n",
    "soup.i.get_text()\n",
    "u'example.com'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以通过参数指定tag的文本内容的分隔符:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nI linked to |example.com|\\n'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# soup.get_text(\"|\")\n",
    "u'\\nI linked to |example.com|\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还可以去除获得文本内容的前后空白:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I linked to|example.com'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# soup.get_text(\"|\", strip=True)\n",
    "u'I linked to|example.com'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者使用 [.stripped_strings](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#strings-stripped-strings) 生成器,获得文本列表后手动处理列表:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I linked to', 'example.com']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[text for text in soup.stripped_strings]\n",
    "# [u'I linked to', u'example.com']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 指定文档解析器\n",
    "\n",
    "如果仅是想要解析HTML文档,只要用文档创建 `BeautifulSoup` 对象就可以了.Beautiful Soup会自动选择一个解析器来解析文档.但是还可以通过参数指定使用那种解析器来解析当前文档.\n",
    "\n",
    "`BeautifulSoup` 第一个参数应该是要被解析的文档字符串或是文件句柄,第二个参数用来标识怎样解析文档.如果第二个参数为空,那么Beautiful Soup根据当前系统安装的库自动选择解析器,解析器的优先数序: lxml, html5lib, Python标准库.在下面两种条件下解析器优先顺序会变化:\n",
    "\n",
    "> - 要解析的文档是什么类型: 目前支持, “html”, “xml”, 和 “html5”\n",
    "> - 指定使用哪种解析器: 目前支持, “lxml”, “html5lib”, 和 “html.parser”\n",
    "\n",
    "[安装解析器](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id13) 章节介绍了可以使用哪种解析器,以及如何安装.\n",
    "\n",
    "如果指定的解析器没有安装,Beautiful Soup会自动选择其它方案.目前只有 lxml 解析器支持XML文档的解析,在没有安装lxml库的情况下,创建 `beautifulsoup` 对象时无论是否指定使用lxml,都无法得到解析后的对象\n",
    "\n",
    "## 解析器之间的区别\n",
    "\n",
    "Beautiful Soup为不同的解析器提供了相同的接口,但解析器本身时有区别的.同一篇文档被不同的解析器解析后可能会生成不同结构的树型文档.区别最大的是HTML解析器和XML解析器,看下面片段被解析成HTML结构:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><body><a><b></b></a></body></html>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BeautifulSoup(\"<a><b /></a>\")\n",
    "# <html><head></head><body><a><b></b></a></body></html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为空标签<b />不符合HTML标准,所以解析器把它解析成<b></b>\n",
    "\n",
    "同样的文档使用XML解析如下(解析XML需要安装lxml库).注意,空标签<b />依然被保留,并且文档前添加了XML头,而不是被包含在<html>标签内:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<a><b/></a>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BeautifulSoup(\"<a><b /></a>\", \"xml\")\n",
    "# <?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
    "# <a><b/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML解析器之间也有区别,如果被解析的HTML文档是标准格式,那么解析器之间没有任何差别,只是解析速度不同,结果都会返回正确的文档树.\n",
    "\n",
    "但是如果被解析文档不是标准格式,那么不同的解析器返回结果可能不同.下面例子中,使用lxml解析错误格式的文档,结果</p>标签被直接忽略掉了:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><body><a></a></body></html>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BeautifulSoup(\"<a></p>\", \"lxml\")\n",
    "# <html><body><a></a></body></html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用html5lib库解析相同文档会得到不同的结果:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><head></head><body><a><p></p></a></body></html>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BeautifulSoup(\"<a></p>\", \"html5lib\")\n",
    "# <html><head></head><body><a><p></p></a></body></html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "html5lib库没有忽略掉</p>标签,而是自动补全了标签,还给文档树添加了<head>标签.\n",
    "\n",
    "使用pyhton内置库解析结果如下:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a></a>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BeautifulSoup(\"<a></p>\", \"html.parser\")\n",
    "# <a></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与lxml [[7\\]](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id95) 库类似的,Python内置库忽略掉了</p>标签,与html5lib库不同的是标准库没有尝试创建符合标准的文档格式或将文档片段包含在<body>标签内,与lxml不同的是标准库甚至连<html>标签都没有尝试去添加.\n",
    "\n",
    "因为文档片段`<a></p>`”`是错误格式,所以以上解析方式都能算作”正确”,html5lib库使用的是HTML5的部分标准,所以最接近”正确”.不过所有解析器的结构都能够被认为是”正常”的.\n",
    "\n",
    "不同的解析器可能影响代码执行结果,如果在分发给别人的代码中使用了 `BeautifulSoup` ,那么最好注明使用了哪种解析器,以减少不必要的麻烦.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 编码\n",
    "\n",
    "任何HTML或XML文档都有自己的编码方式,比如ASCII 或 UTF-8,但是使用Beautiful Soup解析后,文档都被转换成了Unicode:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1>SacrÃ© bleu!</h1>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'SacrÃ© bleu!'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markup = \"<h1>Sacr\\xc3\\xa9 bleu!</h1>\"\n",
    "soup = BeautifulSoup(markup)\n",
    "soup.h1\n",
    "# <h1>Sacré bleu!</h1>\n",
    "soup.h1.string\n",
    "# u'Sacr\\xe9 bleu!'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这不是魔术(但很神奇),Beautiful Soup用了 [编码自动检测](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#unicode-dammit) 子库来识别当前文档编码并转换成Unicode编码. `BeautifulSoup` 对象的 `.original_encoding` 属性记录了自动识别编码的结果:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'utf-8'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.original_encoding\n",
    "'utf-8'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[编码自动检测](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#unicode-dammit) 功能大部分时候都能猜对编码格式,但有时候也会出错.有时候即使猜测正确,也是在逐个字节的遍历整个文档后才猜对的,这样很慢.如果预先知道文档编码,可以设置编码参数来减少自动检查编码出错的概率并且提高文档解析速度.在创建 `BeautifulSoup` 对象的时候设置 `from_encoding` 参数.\n",
    "\n",
    "下面一段文档用了ISO-8859-8编码方式,这段文档太短,结果Beautiful Soup以为文档是用ISO-8859-7编码:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1>νεμω</h1>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'ISO-8859-7'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markup = b\"<h1>\\xed\\xe5\\xec\\xf9</h1>\"\n",
    "soup = BeautifulSoup(markup)\n",
    "soup.h1\n",
    "# <h1>νεμω</h1>\n",
    "soup.original_encoding\n",
    "# 'ISO-8859-7'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过传入 `from_encoding` 参数来指定编码方式:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1>םולש</h1>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'iso-8859-8'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(markup, from_encoding=\"iso-8859-8\")\n",
    "soup.h1\n",
    "# <h1>םולש</h1>\n",
    "soup.original_encoding\n",
    "# 'iso8859-8'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果仅知道文档采用了Unicode编码, 但不知道具体编码. 可以先自己猜测, 猜测错误(依旧是乱码)时, 可以把错误编码作为 `exclude_encodings` 参数, 这样文档就不会尝试使用这种编码了解码了. 译者备注: 在没有指定编码的情况下, BS会自己猜测编码, 把不正确的编码排除掉, BS就更容易猜到正确编码.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1>íåìù</h1>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'windows-1252'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(markup, exclude_encodings=[\"ISO-8859-7\"])\n",
    "soup.h1\n",
    "# <h1>םולש</h1>\n",
    "soup.original_encoding\n",
    "# 'WINDOWS-1255'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "猜测结果是 Windows-1255 编码, 猜测结果可能不够准确, 但是 Windows-1255 编码是 ISO-8859-8 的扩展集, 所以猜测结果已经十分接近了, 并且不影响使用. (`exclude_encodings` 参数是 4.4.0版本的新功能)\n",
    "\n",
    "少数情况下(通常是UTF-8编码的文档中包含了其它编码格式的文件),想获得正确的Unicode编码就不得不将文档中少数特殊编码字符替换成特殊Unicode编码,“REPLACEMENT CHARACTER” (U+FFFD, �) [[9\\]](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id97) . 如果Beautifu Soup猜测文档编码时作了特殊字符的替换,那么Beautiful Soup会把 `UnicodeDammit` 或 `BeautifulSoup` 对象的 `.contains_replacement_characters` 属性标记为 `True` .这样就可以知道当前文档进行Unicode编码后丢失了一部分特殊内容字符.如果文档中包含�而 `.contains_replacement_characters` 属性是 `False` ,则表示�就是文档中原来的字符,不是转码失败.\n",
    "\n",
    "## 输出编码\n",
    "\n",
    "通过Beautiful Soup输出文档时,不管输入文档是什么编码方式,输出编码均为UTF-8编码,下面例子输入文档是Latin-1编码:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-type\"/>\n",
      " </head>\n",
      " <body>\n",
      "  <p>\n",
      "   Sacré bleu!\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "markup = b'''\n",
    "<html>\n",
    "  <head>\n",
    "    <meta content=\"text/html; charset=ISO-Latin-1\" http-equiv=\"Content-type\" />\n",
    "  </head>\n",
    "  <body>\n",
    "    <p>Sacr\\xe9 bleu!</p>\n",
    "  </body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "soup = BeautifulSoup(markup)\n",
    "print(soup.prettify())\n",
    "# <html>\n",
    "#  <head>\n",
    "#   <meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-type\" />\n",
    "#  </head>\n",
    "#  <body>\n",
    "#   <p>\n",
    "#    Sacré bleu!\n",
    "#   </p>\n",
    "#  </body>\n",
    "# </html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意,输出文档中的<meta>标签的编码设置已经修改成了与输出编码一致的UTF-8.\n",
    "\n",
    "如果不想用UTF-8编码输出,可以将编码方式传入 `prettify()` 方法:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<html>\\n <head>\\n  <meta content=\"text/html; charset=latin-1\" http-equiv=\"Content-type\"/>\\n </head>\\n <body>\\n  <p>\\n   Sacr\\xe9 bleu!\\n  </p>\\n </body>\\n</html>\\n'\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify(\"latin-1\"))\n",
    "# <html>\n",
    "#  <head>\n",
    "#   <meta content=\"text/html; charset=latin-1\" http-equiv=\"Content-type\" />\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还可以调用 `BeautifulSoup` 对象或任意节点的 `encode()` 方法,就像Python的字符串调用 `encode()` 方法一样:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<p>Sacr\\xe9 bleu!</p>'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "b'<p>Sacr\\xc3\\xa9 bleu!</p>'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.p.encode(\"latin-1\")\n",
    "# '<p>Sacr\\xe9 bleu!</p>'\n",
    "\n",
    "soup.p.encode(\"utf-8\")\n",
    "# '<p>Sacr\\xc3\\xa9 bleu!</p>'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果文档中包含当前编码不支持的字符,那么这些字符将被转换成一系列XML特殊字符引用,下面例子中包含了Unicode编码字符SNOWMAN:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "markup = u\"<b>\\N{SNOWMAN}</b>\"\n",
    "snowman_soup = BeautifulSoup(markup)\n",
    "tag = snowman_soup.b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SNOWMAN字符在UTF-8编码中可以正常显示(看上去像是☃),但有些编码不支持SNOWMAN字符,比如ISO-Latin-1或ASCII,那么在这些编码中SNOWMAN字符会被转换成“&#9731”:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<b>\\xe2\\x98\\x83</b>'\n",
      "b'<b>&#9731;</b>'\n",
      "b'<b>&#9731;</b>'\n"
     ]
    }
   ],
   "source": [
    "print(tag.encode(\"utf-8\"))\n",
    "# <b>☃</b>\n",
    "\n",
    "print(tag.encode(\"latin-1\"))\n",
    "# <b>&#9731;</b>\n",
    "\n",
    "print(tag.encode(\"ascii\"))\n",
    "# <b>&#9731;</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicode, Dammit! (乱码, 靠!)\n",
    "\n",
    "译者备注: UnicodeDammit 是BS内置库, 主要用来猜测文档编码.\n",
    "\n",
    "[编码自动检测](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#unicode-dammit) 功能可以在Beautiful Soup以外使用,检测某段未知编码时,可以使用这个方法:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SacrÃ© bleu!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import UnicodeDammit\n",
    "dammit = UnicodeDammit(\"Sacr\\xc3\\xa9 bleu!\")\n",
    "print(dammit.unicode_markup)\n",
    "# Sacré bleu!\n",
    "dammit.original_encoding\n",
    "# 'utf-8'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果Python中安装了 `chardet` 或 `cchardet` 那么编码检测功能的准确率将大大提高. 输入的字符越多,检测结果越精确,如果事先猜测到一些可能编码, 那么可以将猜测的编码作为参数,这样将优先检测这些编码:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sacré bleu!\n"
     ]
    }
   ],
   "source": [
    "dammit = UnicodeDammit(\"Sacr\\xe9 bleu!\", [\"latin-1\", \"iso-8859-1\"])\n",
    "print(dammit.unicode_markup)\n",
    "# Sacré bleu!\n",
    "dammit.original_encoding\n",
    "# 'latin-1'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[编码自动检测](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#unicode-dammit) 功能中有2项功能是Beautiful Soup库中用不到的\n",
    "\n",
    "### 智能引号\n",
    "\n",
    "使用Unicode时,Beautiful Soup还会智能的把引号 [[10\\]](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id98) 转换成HTML或XML中的特殊字符:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>I just &ldquo;love&rdquo; Microsoft Word&rsquo;s smart quotes</p>'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'<p>I just &#x201C;love&#x201D; Microsoft Word&#x2019;s smart quotes</p>'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markup = b\"<p>I just \\x93love\\x94 Microsoft Word\\x92s smart quotes</p>\"\n",
    "\n",
    "UnicodeDammit(markup, [\"windows-1252\"], smart_quotes_to=\"html\").unicode_markup\n",
    "# u'<p>I just &ldquo;love&rdquo; Microsoft Word&rsquo;s smart quotes</p>'\n",
    "\n",
    "UnicodeDammit(markup, [\"windows-1252\"], smart_quotes_to=\"xml\").unicode_markup\n",
    "# u'<p>I just &#x201C;love&#x201D; Microsoft Word&#x2019;s smart quotes</p>'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以把引号转换为ASCII码:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>I just \"love\" Microsoft Word\\'s smart quotes</p>'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UnicodeDammit(markup, [\"windows-1252\"], smart_quotes_to=\"ascii\").unicode_markup\n",
    "# u'<p>I just \"love\" Microsoft Word\\'s smart quotes</p>'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "很有用的功能,但是Beautiful Soup没有使用这种方式.默认情况下,Beautiful Soup把引号转换成Unicode:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>I just “love” Microsoft Word’s smart quotes</p>'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UnicodeDammit(markup, [\"windows-1252\"]).unicode_markup\n",
    "# u'<p>I just \\u201clove\\u201d Microsoft Word\\u2019s smart quotes</p>'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矛盾的编码\n",
    "\n",
    "有时文档的大部分都是用UTF-8,但同时还包含了Windows-1252编码的字符,就像微软的智能引号 [[10\\]](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id98) 一样. 一些包含多个信息的来源网站容易出现这种情况. `UnicodeDammit.detwingle()` 方法可以把这类文档转换成纯UTF-8编码格式,看个简单的例子:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowmen = (u\"\\N{SNOWMAN}\" * 3)\n",
    "quote = (u\"\\N{LEFT DOUBLE QUOTATION MARK}I like snowmen!\\N{RIGHT DOUBLE QUOTATION MARK}\")\n",
    "doc = snowmen.encode(\"utf8\") + quote.encode(\"windows_1252\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段文档很杂乱,snowmen是UTF-8编码,引号是Windows-1252编码,直接输出时不能同时显示snowmen和引号,因为它们编码不同:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xe2\\x98\\x83\\xe2\\x98\\x83\\xe2\\x98\\x83\\x93I like snowmen!\\x94'\n",
      "â˜ƒâ˜ƒâ˜ƒ“I like snowmen!”\n"
     ]
    }
   ],
   "source": [
    "print(doc)\n",
    "# ☃☃☃�I like snowmen!�\n",
    "\n",
    "print(doc.decode(\"windows-1252\"))\n",
    "# â˜ƒâ˜ƒâ˜ƒ“I like snowmen!”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对这段文档用UTF-8解码就会得到 `UnicodeDecodeError` 异常,如果用Windows-1252解码就回得到一堆乱码. 幸好, `UnicodeDammit.detwingle()` 方法会把这段字符串转换成UTF-8编码,允许我们同时显示出文档中的snowmen和引号:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☃☃☃“I like snowmen!”\n"
     ]
    }
   ],
   "source": [
    "new_doc = UnicodeDammit.detwingle(doc)\n",
    "print(new_doc.decode(\"utf8\"))\n",
    "# ☃☃☃“I like snowmen!”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`UnicodeDammit.detwingle()` 方法只能解码包含在UTF-8编码中的Windows-1252编码内容,但这解决了最常见的一类问题.\n",
    "\n",
    "在创建 `BeautifulSoup` 或 `UnicodeDammit` 对象前一定要先对文档调用 `UnicodeDammit.detwingle()` 确保文档的编码方式正确.如果尝试去解析一段包含Windows-1252编码的UTF-8文档,就会得到一堆乱码,比如: â˜ƒâ˜ƒâ˜ƒ“I like snowmen!”.\n",
    "\n",
    "`UnicodeDammit.detwingle()` 方法在Beautiful Soup 4.1.0版本中新增\n",
    "\n",
    "# 比较对象是否相同\n",
    "\n",
    "两个 `NavigableString` 或 `Tag` 对象具有相同的HTML或XML结构时, Beautiful Soup就判断这两个对象相同. 这个例子中, 2个 <b> 标签在 BS 中是相同的, 尽管他们在文档树的不同位置, 但是具有相同的表象: “<b>pizza</b>”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "markup = \"<p>I want <b>pizza</b> and more <b>pizza</b>!</p>\"\n",
    "soup = BeautifulSoup(markup, 'html.parser')\n",
    "first_b, second_b = soup.find_all('b')\n",
    "print(first_b == second_b)\n",
    "# True\n",
    "\n",
    "print(first_b.previous_element == second_b.previous_element)\n",
    "# False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想判断两个对象是否严格的指向同一个对象可以通过 `is` 来判断\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(first_b is second_b)\n",
    "# False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 复制Beautiful Soup对象\n",
    "\n",
    "`copy.copy()` 方法可以复制任意 `Tag` 或 `NavigableString` 对象\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>I want <b>pizza</b> and more <b>pizza</b>!</p>\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "p_copy = copy.copy(soup.p)\n",
    "print(p_copy)\n",
    "# <p>I want <b>pizza</b> and more <b>pizza</b>!</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "复制后的对象跟与对象是相等的, 但指向不同的内存地址\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(soup.p == p_copy)\n",
    "# True\n",
    "\n",
    "print(soup.p is p_copy)\n",
    "# False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "源对象和复制对象的区别是源对象在文档树中, 而复制后的对象是独立的还没有添加到文档树中. 复制后对象的效果跟调用了 `extract()` 方法相同.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(p_copy.parent)\n",
    "# None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是因为相等的对象不能同时插入相同的位置\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 解析部分文档\n",
    "\n",
    "如果仅仅因为想要查找文档中的`<a>`标签而将整片文档进行解析,实在是浪费内存和时间.最快的方法是从一开始就把`<a>`标签以外的东西都忽略掉. `SoupStrainer` 类可以定义文档的某段内容,这样搜索文档时就不必先解析整篇文档,只会解析在 `SoupStrainer` 中定义过的文档. 创建一个 `SoupStrainer` 对象并作为 `parse_only` 参数给 `BeautifulSoup` 的构造方法即可.\n",
    "\n",
    "## SoupStrainer\n",
    "\n",
    "`SoupStrainer` 类接受与典型搜索方法相同的参数：[name](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id36) , [attrs](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#css) , [recursive](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#recursive) , [string](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id37) , [**kwargs](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#keyword) 。下面举例说明三种 `SoupStrainer` 对象：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import SoupStrainer\n",
    "\n",
    "only_a_tags = SoupStrainer(\"a\")\n",
    "\n",
    "only_tags_with_id_link2 = SoupStrainer(id=\"link2\")\n",
    "\n",
    "def is_short_string(string):\n",
    "    return len(string) < 10\n",
    "\n",
    "only_short_strings = SoupStrainer(string=is_short_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再拿“爱丽丝”文档来举例，来看看使用三种 `SoupStrainer` 对象做参数会有什么不同:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n",
      " Elsie\n",
      "</a>\n",
      "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\n",
      " Lacie\n",
      "</a>\n",
      "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">\n",
      " Tillie\n",
      "</a>\n",
      "\n",
      "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\n",
      " Lacie\n",
      "</a>\n",
      "\n",
      "Elsie\n",
      ",\n",
      "Lacie\n",
      "and\n",
      "Tillie\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "html_doc = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "    <body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\"\n",
    "\n",
    "print(BeautifulSoup(html_doc, \"html.parser\", parse_only=only_a_tags).prettify())\n",
    "# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n",
    "#  Elsie\n",
    "# </a>\n",
    "# <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\n",
    "#  Lacie\n",
    "# </a>\n",
    "# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">\n",
    "#  Tillie\n",
    "# </a>\n",
    "\n",
    "print(BeautifulSoup(html_doc, \"html.parser\", parse_only=only_tags_with_id_link2).prettify())\n",
    "# <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\n",
    "#  Lacie\n",
    "# </a>\n",
    "\n",
    "print(BeautifulSoup(html_doc, \"html.parser\", parse_only=only_short_strings).prettify())\n",
    "# Elsie\n",
    "# ,\n",
    "# Lacie\n",
    "# and\n",
    "# Tillie\n",
    "# ...\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还可以将 `SoupStrainer` 作为参数传入 [搜索文档树](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id28) 中提到的方法.这可能不是个常用用法,所以还是提一下:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Elsie',\n",
       " ',\\n',\n",
       " 'Lacie',\n",
       " ' and\\n',\n",
       " 'Tillie',\n",
       " '\\n',\n",
       " '...',\n",
       " '\\n']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(html_doc)\n",
    "soup.find_all(only_short_strings)\n",
    "# [u'\\n\\n', u'\\n\\n', u'Elsie', u',\\n', u'Lacie', u' and\\n', u'Tillie',\n",
    "#  u'\\n\\n', u'...', u'\\n']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 常见问题\n",
    "\n",
    "## 代码诊断\n",
    "\n",
    "如果想知道Beautiful Soup到底怎样处理一份文档,可以将文档传入 `diagnose()` 方法(Beautiful Soup 4.2.0中新增),Beautiful Soup会输出一份报告,说明不同的解析器会怎样处理这段文档,并标出当前的解析过程会使用哪种解析器:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnostic running on Beautiful Soup 4.12.2\n",
      "Python version 3.11.6 (tags/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)]\n",
      "Found lxml version 4.9.3.0\n",
      "Found html5lib version 1.1\n",
      "Trying to parse your markup with html.parser\n",
      "Here's what html.parser did with the markup:\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/>\n",
      "  <title>\n",
      "   Document\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      " </body>\n",
      "</html>\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Trying to parse your markup with html5lib\n",
      "Here's what html5lib did with the markup:\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/>\n",
      "  <title>\n",
      "   Document\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      " </body>\n",
      "</html>\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Trying to parse your markup with lxml\n",
      "Here's what lxml did with the markup:\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/>\n",
      "  <title>\n",
      "   Document\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      " </body>\n",
      "</html>\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Trying to parse your markup with lxml-xml\n",
      "Here's what lxml-xml did with the markup:\n",
      "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"UTF-8\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/>\n",
      "  <title>\n",
      "   Document\n",
      "  </title>\n",
      " </head>\n",
      " <body/>\n",
      "</html>\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from bs4.diagnose import diagnose\n",
    "data = open(\"index.html\").read()\n",
    "diagnose(data)\n",
    "\n",
    "# Diagnostic running on Beautiful Soup 4.2.0\n",
    "# Python version 2.7.3 (default, Aug  1 2012, 05:16:07)\n",
    "# I noticed that html5lib is not installed. Installing it may help.\n",
    "# Found lxml version 2.3.2.0\n",
    "#\n",
    "# Trying to parse your data with html.parser\n",
    "# Here's what html.parser did with the document:\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`diagnose()` 方法的输出结果可能帮助你找到问题的原因,如果不行,还可以把结果复制出来以便寻求他人的帮助\n",
    "\n",
    "## 文档解析错误\n",
    "\n",
    "文档解析错误有两种.一种是崩溃,Beautiful Soup尝试解析一段文档结果却抛除了异常,通常是 `HTMLParser.HTMLParseError` .还有一种异常情况,是Beautiful Soup解析后的文档树看起来与原来的内容相差很多.\n",
    "\n",
    "这些错误几乎都不是Beautiful Soup的原因,这不会是因为Beautiful Soup的代码写的太优秀,而是因为Beautiful Soup没有包含任何文档解析代码.异常产生自被依赖的解析器,如果解析器不能很好的解析出当前的文档,那么最好的办法是换一个解析器.更多细节查看 [安装解析器](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id13) 章节.\n",
    "\n",
    "最常见的解析错误是 `HTMLParser.HTMLParseError: malformed start tag` 和 `HTMLParser.HTMLParseError: bad end tag` .这都是由Python内置的解析器引起的,解决方法是 [安装lxml或html5lib](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id13)\n",
    "\n",
    "最常见的异常现象是当前文档找不到指定的Tag,而这个Tag光是用眼睛就足够发现的了. `find_all()` 方法返回 [] ,而 `find()` 方法返回 None .这是Python内置解析器的又一个问题: 解析器会跳过那些它不知道的tag.解决方法还是 [安装lxml或html5lib](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id13)\n",
    "\n",
    "## 版本错误\n",
    "\n",
    "- `SyntaxError: Invalid syntax` (异常位置在代码行: `ROOT_TAG_NAME = u'[document]'` ),因为Python2语法的代码(没有经过迁移)直接在Python3中运行\n",
    "- `ImportError: No module named HTMLParser` 因为在Python3中执行Python2版本的Beautiful Soup\n",
    "- `ImportError: No module named html.parser` 因为在Python2中执行Python3版本的Beautiful Soup\n",
    "- `ImportError: No module named BeautifulSoup` 因为在没有安装BeautifulSoup3库的Python环境下执行代码,或忘记了BeautifulSoup4的代码需要从 `bs4` 包中引入\n",
    "- `ImportError: No module named bs4` 因为当前Python环境下还没有安装BeautifulSoup4\n",
    "\n",
    "## 解析成XML\n",
    "\n",
    "默认情况下,Beautiful Soup会将当前文档作为HTML格式解析,如果要解析XML文档,要在 `BeautifulSoup` 构造方法中加入第二个参数 “xml”:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(markup, \"xml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然,还需要 [安装lxml](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id13)\n",
    "\n",
    "## 解析器的错误\n",
    "\n",
    "- 如果同样的代码在不同环境下结果不同,可能是因为两个环境下使用不同的解析器造成的.例如这个环境中安装了lxml,而另一个环境中只有html5lib, [解析器之间的区别](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id54) 中说明了原因.修复方法是在 `BeautifulSoup` 的构造方法中中指定解析器\n",
    "- 因为HTML标签是 [大小写敏感](http://www.w3.org/TR/html5/syntax.html#syntax) 的,所以3种解析器再出来文档时都将tag和属性转换成小写.例如文档中的 <TAG></TAG> 会被转换为 <tag></tag> .如果想要保留tag的大写的话,那么应该将文档 [解析成XML](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#xml) .\n",
    "\n",
    "## 杂项错误\n",
    "\n",
    "- `UnicodeEncodeError: 'charmap' codec can't encode character u'\\xfoo' in position bar` (或其它类型的 `UnicodeEncodeError` )的错误,主要是两方面的错误(都不是Beautiful Soup的原因),第一种是正在使用的终端(console)无法显示部分Unicode,参考 [Python wiki](http://wiki.python.org/moin/PrintFails) ,第二种是向文件写入时,被写入文件不支持部分Unicode,这时只要用 `u.encode(\"utf8\")` 方法将编码转换为UTF-8.\n",
    "- `KeyError: [attr]` 因为调用 `tag['attr']` 方法而引起,因为这个tag没有定义该属性.出错最多的是 `KeyError: 'href'` 和 `KeyError: 'class'` .如果不确定某个属性是否存在时,用 `tag.get('attr')` 方法去获取它,跟获取Python字典的key一样\n",
    "- `AttributeError: 'ResultSet' object has no attribute 'foo'` 错误通常是因为把 `find_all()` 的返回结果当作一个tag或文本节点使用,实际上返回结果是一个列表或 `ResultSet` 对象的字符串,需要对结果进行循环才能得到每个节点的 `.foo` 属性.或者使用 `find()` 方法仅获取到一个节点\n",
    "- `AttributeError: 'NoneType' object has no attribute 'foo'` 这个错误通常是在调用了 `find()` 方法后直节点取某个属性 .foo 但是 `find()` 方法并没有找到任何结果,所以它的返回值是 `None` .需要找出为什么 `find()` 的返回值是 `None` .\n",
    "\n",
    "## 如何提高效率\n",
    "\n",
    "Beautiful Soup对文档的解析速度不会比它所依赖的解析器更快,如果对计算时间要求很高或者计算机的时间比程序员的时间更值钱,那么就应该直接使用 [lxml](http://lxml.de/) .\n",
    "\n",
    "换句话说,还有提高Beautiful Soup效率的办法,使用lxml作为解析器.Beautiful Soup用lxml做解析器比用html5lib或Python内置解析器速度快很多.\n",
    "\n",
    "安装 [cchardet](http://pypi.python.org/pypi/cchardet/) 后文档的解码的编码检测会速度更快\n",
    "\n",
    "[解析部分文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id65) 不会节省多少解析时间,但是会节省很多内存,并且搜索时也会变得更快.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for parent in tag.parentGenerator():\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "替换为:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for parent in tag.parents:\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(两种调用方法现在都能使用)\n",
    "\n",
    "BS3中有的生成器循环结束后会返回 `None` 然后结束.这是个bug.新版生成器不再返回 `None` .\n",
    "\n",
    "BS4中增加了2个新的生成器, [.strings 和 stripped_strings](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#strings-stripped-strings) . `.strings` 生成器返回NavigableString对象, `.stripped_strings` 方法返回去除前后空白的Python的string对象.\n",
    "\n",
    "### XML\n",
    "\n",
    "BS4中移除了解析XML的 `BeautifulStoneSoup` 类.如果要解析一段XML文档,使用 `BeautifulSoup` 构造方法并在第二个参数设置为“xml”.同时 `BeautifulSoup` 构造方法也不再识别 `isHTML` 参数.\n",
    "\n",
    "Beautiful Soup处理XML空标签的方法升级了.旧版本中解析XML时必须指明哪个标签是空标签. 构造方法的 `selfClosingTags` 参数已经不再使用.新版Beautiful Soup将所有空标签解析为空元素,如果向空元素中添加子节点,那么这个元素就不再是空元素了.\n",
    "\n",
    "### 实体\n",
    "\n",
    "HTML或XML实体都会被解析成Unicode字符,Beautiful Soup 3版本中有很多处理实体的方法,在新版中都被移除了. `BeautifulSoup` 构造方法也不再接受 `smartQuotesTo` 或 `convertEntities` 参数. [编码自动检测](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#unicode-dammit) 方法依然有 `smart_quotes_to` 参数,但是默认会将引号转换成Unicode.内容配置项 `HTML_ENTITIES` , `XML_ENTITIES` 和 `XHTML_ENTITIES` 在新版中被移除.因为它们代表的特性已经不再被支持.\n",
    "\n",
    "如果在输出文档时想把Unicode字符转换成HTML实体,而不是输出成UTF-8编码,那就需要用到 [输出格式](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id52) 的方法.\n",
    "\n",
    "### 迁移杂项\n",
    "\n",
    "[Tag.string](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#string) 属性现在是一个递归操作.如果A标签只包含了一个B标签,那么A标签的.string属性值与B标签的.string属性值相同.\n",
    "\n",
    "[多值属性](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id16) 比如 `class` 属性包含一个他们的值的列表,而不是一个字符串.这可能会影响到如何按照CSS类名哦搜索tag.\n",
    "\n",
    "如果使用 `find*` 方法时同时传入了 [string 参数](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id37) 和 [name 参数](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id36) .Beautiful Soup会搜索指定name的tag,并且这个tag的 [Tag.string](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#string) 属性包含text参数的内容.结果中不会包含字符串本身.旧版本中Beautiful Soup会忽略掉tag参数,只搜索text参数.\n",
    "\n",
    "`BeautifulSoup` 构造方法不再支持 markupMassage 参数.现在由解析器负责文档的解析正确性.\n",
    "\n",
    "很少被用到的几个解析器方法在新版中被移除,比如 `ICantBelieveItsBeautifulSoup` 和 `BeautifulSOAP` .现在由解析器完全负责如何解释模糊不清的文档标记.\n",
    "\n",
    "`prettify()` 方法在新版中返回Unicode字符串,不再返回字节流.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 附录\n",
    "\n",
    "\n",
    "> [1]\tBeautifulSoup的google讨论组不是很活跃,可能是因为库已经比较完善了吧,但是作者还是会很热心的尽量帮你解决问题的.\n",
    "> \n",
    "> [2]\t(1, 2) 文档被解析成树形结构,所以下一步解析过程应该是当前节点的子节点\n",
    "> \n",
    "> [3]\t过滤器只能作为搜索文档的参数,或者说应该叫参数类型更为贴切,原文中用了 filter 因此翻译为过滤器\n",
    "> \n",
    "> [4]\t元素参数,HTML文档中的一个tag节点,不能是文本节点\n",
    "> \n",
    "> [5]\t(1, 2, 3, 4, 5) 采用先序遍历方式\n",
    "> \n",
    "> [6]\t(1, 2) CSS选择器是一种单独的文档搜索语法, 参考 http://www.w3school.com.cn/css/css_selector_type.asp\n",
    "> \n",
    "> [7]\t原文写的是 html5lib, 译者觉得这是原文档的一个笔误\n",
    "> \n",
    "> [8]\twrap含有包装,打包的意思,但是这里的包装不是在外部包装而是将当前tag的内部内容包装在一个tag里.包装原来内容的新tag依然在执行 wrap() 方法的tag内\n",
    "> \n",
    "> [9]\t文档中特殊编码字符被替换成特殊字符(通常是�)的过程是Beautful Soup自动实现的,如果想要多种编码格式的文档被完全转换正确,那么,只好,预先手动处理,统一编码格式\n",
    "> \n",
    "> [10]\t(1, 2) 智能引号,常出现在microsoft的word软件中,即在某一段落中按引号出现的顺序每个引号都被自动转换为左引号,或右引号.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
